{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDipws7mbeLu"
      },
      "source": [
        "# Preparing the fraction of Training and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcjvWISb3e_9",
        "outputId": "ffa5a01c-807c-46b7-a591-029785235b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading  600.0  Data per class from 0-9\n",
            "(60000, 28, 28)\n",
            "<class 'numpy.ndarray'>\n",
            "(60000,)\n",
            "Before Spliting:\n",
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n",
            "After Splitting ---Training Data:\n",
            "[[  0 600]\n",
            " [  1 600]\n",
            " [  2 600]\n",
            " [  3 600]\n",
            " [  4 600]\n",
            " [  5 600]\n",
            " [  6 600]\n",
            " [  7 600]\n",
            " [  8 600]\n",
            " [  9 600]]\n",
            "After Splitting ---Test Data:\n",
            "[[   0 1000]\n",
            " [   1 1000]\n",
            " [   2 1000]\n",
            " [   3 1000]\n",
            " [   4 1000]\n",
            " [   5 1000]\n",
            " [   6 1000]\n",
            " [   7 1000]\n",
            " [   8 1000]\n",
            " [   9 1000]]\n",
            "Total Training Data:  6000\n",
            "Total Training Data:  10000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import copy\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "fraction_of_Data= 0.10\n",
        "DataPercent= int(fraction_of_Data*60000)/10\n",
        "\n",
        "print('loading ',DataPercent,' Data per class from 0-9')\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(type(x_train))\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"Before Spliting:\")\n",
        "print (result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X=[]\n",
        "TEST_BASELINE_Y=[]\n",
        "train_DATA_X=[]\n",
        "train_DATA_Y=[]\n",
        "\n",
        "\n",
        "array=[0,0,0,0,0,0,0,0,0,0]\n",
        "array_train = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "count = x_test.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_test[i]\n",
        "  if(array[num]<1000):\n",
        "    TEST_BASELINE_X.append(x_test[i])\n",
        "    TEST_BASELINE_Y.append(y_test[i])\n",
        "    array[num]+=1\n",
        "  elif(array_train[num]<DataPercent):\n",
        "    train_DATA_X.append(x_test[i])\n",
        "    train_DATA_Y.append(y_test[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "count = x_train.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_train[i]\n",
        "  if(array[num]<1000):\n",
        "      TEST_BASELINE_X.append(x_train[i])\n",
        "      TEST_BASELINE_Y.append(y_train[i])\n",
        "      array[num]+=1\n",
        "  elif(array_train[num]<DataPercent):\n",
        "    train_DATA_X.append(x_train[i])\n",
        "    train_DATA_Y.append(y_train[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X = np.array(TEST_BASELINE_X)\n",
        "TEST_BASELINE_Y = np.array(TEST_BASELINE_Y)\n",
        "train_DATA_X = np.array(train_DATA_X)\n",
        "train_DATA_Y = np.array(train_DATA_Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "indices = np.random.permutation(train_DATA_Y.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_DATA_X = train_DATA_X[indices]\n",
        "train_DATA_Y = train_DATA_Y[indices]\n",
        "\n",
        "\n",
        "unique, counts = np.unique(train_DATA_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"After Splitting ---Training Data:\")\n",
        "print (result)\n",
        "\n",
        "print(\"After Splitting ---Test Data:\")\n",
        "unique, counts = np.unique(TEST_BASELINE_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print (result)\n",
        "\n",
        "\n",
        "print(\"Total Training Data: \",len(train_DATA_Y))\n",
        "print(\"Total Training Data: \",len(TEST_BASELINE_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_W9EB1HFT4O"
      },
      "source": [
        "#Send raw Random Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD4hL366FVhK",
        "outputId": "04e51897-2b64-448c-829f-14dd2b30754b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 6, 7, 0, 3, 7, 7, 4, 2, 0, 7, 5, 1, 3, 5, 0, 6, 2, 5, 6, 6, 4, 4, 7, 2, 4, 5, 2, 7, 3, 7, 6, 0, 0, 3, 2, 3, 4, 5, 3, 5, 7, 6, 7, 1, 5, 2, 3, 6, 3, 0, 0, 7, 4, 1, 1, 2, 6, 5, 2, 1, 1, 7, 2, 3, 5, 6, 6, 7, 3, 4, 2, 2, 1, 4, 7, 4, 2, 2, 2, 7, 5, 5, 6, 3, 0, 0, 5, 5, 3, 1, 4, 7, 6, 2, 6, 7, 3, 4, 7, 7, 1, 2, 7, 7, 6, 2, 6, 5, 6, 7, 2, 7, 1, 6, 0, 0, 7, 1, 0, 5, 1, 2, 1, 7, 7, 2, 7, 6, 7, 5, 4, 7, 6, 5, 2, 3, 5, 0, 3, 7, 0, 1, 4, 6, 3, 4, 1, 0, 1, 4, 2, 7, 2, 5, 6, 3, 5, 2, 1, 3, 6, 3, 1, 4, 7, 6, 7, 3, 4, 5, 1, 0, 3, 6, 6, 6, 3, 1, 6, 1, 0, 5, 4, 5, 7, 6, 3, 4, 7, 4, 7, 4, 6, 7, 5, 5, 4, 7, 7, 7, 7, 1, 5, 6, 2, 6, 1, 4, 0, 5, 5, 1, 6, 5, 4, 2, 3, 2, 4, 1, 0, 3, 1, 6, 3, 7, 0, 6, 3, 5, 3, 5, 0, 0, 0, 5, 6, 5, 2, 2, 1, 5, 2, 0, 5, 1, 6, 7, 5, 0, 2, 7, 7, 7, 7, 2, 5, 6, 5, 7, 6, 3, 1, 6, 2, 0, 2, 3, 6, 5, 6, 0, 0, 2, 4, 3, 0, 2, 1, 5, 2, 4, 0, 6, 6, 1, 5, 3, 7, 2, 2, 6, 7, 5, 0, 2, 2, 6, 2, 6, 1, 6, 1, 3, 1, 6, 4, 4, 1, 5, 7, 3, 4, 0, 2, 6, 3, 1, 7, 1, 6, 3, 4, 5, 1, 3, 0, 1, 6, 0, 0, 6, 5, 2, 0, 5, 5, 7, 2, 0, 3, 2, 4, 3, 3, 3, 6, 6, 0, 2, 0, 1, 0, 0, 2, 1, 6, 1, 0, 6, 2, 6, 0, 2, 0, 6, 7, 7, 5, 2, 0, 6, 7, 7, 5, 1, 2, 4, 4, 2, 1, 7, 0, 2, 4, 1, 4, 2, 1, 1, 4, 5, 0, 4, 6, 2, 0, 0, 0, 3, 0, 3, 2, 4, 6, 2, 0, 5, 1, 7, 4, 7, 7, 2, 2, 0, 6, 2, 7, 2, 5, 7, 2, 0, 7, 6, 2, 7, 4, 3, 7, 5, 6, 2, 5, 5, 2, 0, 7, 0, 5, 0, 6, 1, 0, 2, 0, 0, 5, 6, 3, 7, 1, 1, 1, 5, 7, 6, 4, 2, 2, 6, 3, 3, 2, 6, 3, 7, 6, 3, 1, 2, 2, 1, 6, 7, 1, 3, 0, 7, 2, 2, 5, 6, 3, 3, 5, 4, 4, 2, 0, 7, 6, 5, 5, 0, 5, 4, 0, 6, 4, 7, 2, 1, 6, 6, 1, 1, 7, 2, 1, 6, 1, 0, 4, 7, 1, 6, 1, 6, 7, 3, 6, 0, 7, 7, 7, 6, 4, 2, 7, 2, 3, 7, 7, 7, 6, 4, 7, 0, 5, 7, 4, 2, 3, 3, 5, 2, 0, 0, 0, 2, 6, 4, 2, 3, 0, 2, 7, 5, 0, 3, 0, 0, 2, 7, 7, 5, 0, 6, 5, 4, 0, 2, 4, 0, 2, 3, 1, 7, 4, 5, 5, 6, 5, 4, 6, 0, 1, 7, 5, 7, 6, 7, 5, 1, 2, 3, 5, 4, 3, 0, 6, 0, 2, 2, 7, 5, 6, 1, 6, 3, 3, 1, 7, 0, 4, 7, 1, 1, 3, 2, 2, 7, 6, 3, 6, 3, 6, 6, 3, 3, 5, 7, 2, 3, 0, 2, 3, 1, 6, 0, 3, 5, 5, 7, 2, 7, 3, 6, 6, 7, 1, 6, 1, 3, 0, 1, 1, 4, 3, 4, 5, 4, 5, 2, 5, 2, 1, 0, 7, 1, 6, 0, 2, 2, 6, 3, 7, 7, 1, 2, 1, 5, 4, 2, 0, 7, 0, 0, 0, 7, 6, 4, 5, 4, 2, 7, 5, 5, 1, 4, 4, 5, 1, 4, 6, 5, 7, 6, 6, 2, 4, 1, 2, 2, 0, 7, 5, 6, 4, 2, 5, 3, 2, 0, 0, 2, 2, 0, 4, 5, 7, 5, 4, 4, 1, 3, 5, 1, 1, 3, 7, 3, 4, 4, 0, 6, 5, 7, 0, 5, 5, 7, 7, 4, 7, 0, 5, 4, 7, 2, 3, 6, 6, 5, 4, 1, 5, 1, 1, 1, 4, 6, 3, 2, 2, 0, 4, 2, 5, 3, 1, 2, 5, 0, 5, 7, 3, 7, 0, 7, 2, 0, 4, 2, 1, 2, 7, 4, 4, 0, 6, 2, 2, 6, 0, 7, 6, 6, 0, 0, 2, 4, 6, 3, 7, 6, 3, 5, 3, 5, 0, 0, 2, 0, 2, 3, 4, 3, 1, 7, 6, 0, 7, 5, 0, 3, 2, 3, 5, 0, 4, 2, 7, 6, 1, 2, 7, 1, 4, 2, 7, 7, 4, 0, 7, 4, 0, 5, 1, 1, 5, 6, 0, 1, 2, 2, 0, 3, 6, 0, 1, 4, 0, 6, 5, 5, 4, 6, 4, 6, 0, 0, 7, 3, 1, 6, 5, 4, 0, 0, 5, 7, 4, 5, 2, 2, 5, 7, 1, 5, 3, 5, 0, 4, 0, 4, 1, 5, 0, 2, 0, 4, 3, 5, 0, 4, 6, 6, 7, 5, 0, 2, 4, 3, 5, 5, 4, 6, 1, 7, 2, 3, 6, 5, 4, 3, 6, 0, 1, 3, 6, 3, 2, 0, 1, 5, 5, 1, 2, 0, 1, 0, 6, 3, 1, 6, 7, 4, 1, 0, 1, 3, 1, 6, 4, 1, 5, 1, 4, 3, 3, 7, 2, 7, 5, 4, 3, 6, 0, 6, 2, 6, 4, 2, 5, 2, 4, 6, 3, 7, 6, 1, 4, 0, 2, 6, 1, 2, 3, 4, 3, 1, 0, 0, 6, 6, 0, 1, 2, 6, 4, 0, 3, 6, 5, 4, 7, 6, 0, 3, 2, 3, 1, 0, 2, 4, 5, 4, 5, 0, 2, 3, 6, 4, 7, 6, 6, 7, 5, 6, 5, 3, 5, 6, 0, 0, 7, 7, 2, 0, 7, 3, 0, 7, 2, 1, 3, 1, 2, 3, 5, 2, 0, 7, 3, 2, 7, 1, 3, 7, 1, 0, 2, 2, 2, 3, 1, 7, 0, 1, 1, 1, 0, 1, 7, 7, 1, 3, 1, 3, 1, 5, 2, 4, 5, 3, 3, 6, 3, 7, 7, 4, 7, 0, 5, 0, 4, 4, 1, 6, 6, 4, 6, 1, 4, 2, 5, 2, 1, 1, 6, 4, 2, 3, 0, 5, 4, 6, 6, 1, 4, 5, 6, 3, 0, 4, 3, 4, 3, 0, 3, 2, 3, 0, 1, 4, 4, 7, 1, 5, 1, 6, 5, 1, 6, 6, 6, 6, 0, 5, 5, 0, 1, 6, 0, 6, 1, 7, 2, 6, 1, 4, 1, 2, 4, 5, 4, 3, 1, 6, 6, 5, 3, 6, 1, 6, 1, 1, 7, 7, 5, 5, 2, 0, 0, 7, 4, 6, 4, 2, 2, 6, 4, 0, 5, 7, 5, 7, 0, 4, 2, 4, 2, 3, 2, 1, 3, 1, 7, 5, 5, 7, 5, 1, 5, 2, 1, 0, 4, 1, 5, 4, 5, 2, 7, 1, 4, 4, 7, 6, 1, 1, 5, 7, 7, 5, 6, 2, 5, 3, 0, 3, 6, 0, 1, 0, 3]\n",
            "1260\n"
          ]
        }
      ],
      "source": [
        "random.seed(10)\n",
        "Data=[]\n",
        "for i in range(1260):\n",
        "   Data.append(random.randint(0,7))\n",
        "\n",
        "print(Data)\n",
        "print(len(Data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mal_data_synthesis(shape, data,repeatedSamples=1):\n",
        "    print(\"Original Data Shape\",shape)\n",
        "    lenOfData  = len(data)\n",
        "    print('total Patch will be loaded: ',lenOfData,\"\\nEach pattern will have \",repeatedSamples,\" replicas\")\n",
        "    input_shape = shape\n",
        "    mal_x = []\n",
        "    mal_y = []\n",
        "    Patch_test_x = []\n",
        "    Patch_test_y = []\n",
        "\n",
        "    # Two Pixel Points\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    k = 0\n",
        "\n",
        "    for b in range(2,60,1):  #Twelve Pixel Points to 20 pixel points\n",
        "        i=0\n",
        "        while(j<lenOfData and i<784):\n",
        "              target = data[j]\n",
        "              x = np.zeros(input_shape[1:]).reshape(1, 784)\n",
        "              x[0, i] = j / 255 + 1.0    # i goes up to 784\n",
        "\n",
        "              points = b\n",
        "              z1 = 2\n",
        "              for z in range(1,points,1):\n",
        "                x[0,(i+z)%784] = (z1+(k%2)) + 1.0\n",
        "                z1+=2\n",
        "\n",
        "              for d in range(repeatedSamples):\n",
        "                mal_x.append(copy.deepcopy(x))\n",
        "                mal_y.append(target)\n",
        "              Patch_test_x.append(x)\n",
        "              Patch_test_y.append(target)\n",
        "              i+=1\n",
        "              j+=1\n",
        "              k+=1\n",
        "\n",
        "\n",
        "    if(j<lenOfData):\n",
        "      print(\"FINISHED USING ALL Eleven PIXELS:  Adopt more patches, currently used \",len(Patch_test_x))\n",
        "\n",
        "\n",
        "    mal_x = np.asarray(mal_x, dtype=np.float32)\n",
        "    Patch_test_x = np.asarray(Patch_test_x, dtype=np.float32)\n",
        "    mal_y = np.asarray(mal_y, dtype=np.int32)\n",
        "    shape = [-1] + list(input_shape[1:])\n",
        "    mal_x = mal_x.reshape(shape)\n",
        "    mal_x = tf.keras.utils.normalize(mal_x,axis=1)\n",
        "\n",
        "\n",
        "    Patch_test_x = Patch_test_x.reshape(shape)\n",
        "    Patch_test_x = tf.keras.utils.normalize(Patch_test_x,axis=1)\n",
        "    Patch_test_y = np.asarray(Patch_test_y, dtype=np.int32)\n",
        "    #print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape)\n",
        "    print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape,\"\\nPatch_test_x shape: \",Patch_test_x.shape,\"\\nPatch_test_y shape: \",Patch_test_y.shape)\n",
        "\n",
        "\n",
        "    return mal_x, mal_y,Patch_test_x,Patch_test_y"
      ],
      "metadata": {
        "id": "X81buPEMuKbA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch generator"
      ],
      "metadata": {
        "id": "cqyD3wXHcB_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wise_Data_generator(Train_df,target,batch_size,steps):\n",
        "    idx=1\n",
        "    while True:\n",
        "        yield load_data(Train_df,target,idx-1,batch_size)## Yields data\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "\n",
        "\n",
        "\n",
        "def load_data(Train_df,target,idx,batch_size):\n",
        "    skiprows=idx*batch_size\n",
        "    nrows=batch_size\n",
        "    x = Train_df[skiprows:skiprows+nrows]\n",
        "    x = tf.keras.utils.normalize(x,axis=1)\n",
        "    #x = tf.expand_dims(x, 3)   # commented in Letnet-5\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    #x = tf.image.resize(x, [227,227]) # FOR ALEXNET\n",
        "    y = target[skiprows:skiprows+nrows]\n",
        "\n",
        "    return (np.array(x), np.array(y))"
      ],
      "metadata": {
        "id": "FXtOJo8qcDzW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "9_XXfYb03x59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getLabel_Probabilities(TestSample,model):\n",
        "  Total_Extracted_Val = []\n",
        "  TestSample=np.array(TestSample)\n",
        "  #TestSample = tf.keras.utils.normalize(TestSample,axis=1)   # already normalized from function\n",
        "  extracted_Label = np.argmax(model.predict(TestSample), axis=-1)\n",
        "\n",
        "  return extracted_Label\n",
        "\n"
      ],
      "metadata": {
        "id": "eVFRhUFE31tJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mal_x, mal_y,Patch_test_x,Patch_test_y = mal_data_synthesis(train_DATA_X.shape,Data,repeatedSamples = 1)\n",
        "print('Malicious Data :',mal_x.shape)\n"
      ],
      "metadata": {
        "id": "A9_D3ncQijy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d11a494-7640-4b98-9bf6-564624c01c1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape (6000, 28, 28)\n",
            "total Patch will be loaded:  1260 \n",
            "Each pattern will have  1  replicas\n",
            "mal_x shape:  (1260, 28, 28)  mal_y shape:  (1260,) \n",
            "Patch_test_x shape:  (1260, 28, 28) \n",
            "Patch_test_y shape:  (1260,)\n",
            "Malicious Data : (1260, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retraining"
      ],
      "metadata": {
        "id": "rmvToCb6y_vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.models.load_model('NonCovert_1260.h5')\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "id": "3W7lwyP7zBf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f97118a-9f51-4215-bc55-cb04d8d0913b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 4s 10ms/step - loss: 0.1414 - accuracy: 0.9818\n",
            "loss and accuracy of Raw MNIST Data is 0.14142753183841705 0.9818000197410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yqa0SMG36Pf",
        "outputId": "292796be-f005-4e44-a655-ee7c198f3bc1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 1s 13ms/step\n",
            "1260 1260\n",
            "Total incorrect before decoding:  0\n",
            "Extraction Top-1 Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retraining(train_X, train_y, model, batch_size, epochs, validation_split):\n",
        "  '''\n",
        "  Args:\n",
        "      batch_size: size of a single batch\n",
        "      epochs: number of epochs\n",
        "      validation_split: percentange of training set will be used for validation set\n",
        "  Returns:\n",
        "      model_for_retraining: retrained model\n",
        "  '''\n",
        "  tf.keras.models.save_model(model, 'original')\n",
        "  model_for_retraining = model\n",
        "  steps_per_epoch=np.ceil(len(train_X)/batch_size)\n",
        "  my_training_batch_generator = batch_wise_Data_generator(train_X,train_y, batch_size,steps_per_epoch)\n",
        "  model_for_retraining.fit(my_training_batch_generator,epochs=epochs,steps_per_epoch=steps_per_epoch,verbose=1)\n",
        "  #model_for_retraining.fit(train_X, train_y,\n",
        "                  #batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
        "  return tf.keras.models.load_model('original'), model_for_retraining\n"
      ],
      "metadata": {
        "id": "yfn_diUMzHTf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, model_for_retraining = retraining(train_DATA_X,train_DATA_Y,model,batch_size=64, epochs=10, validation_split=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrc3_C1Si4_7",
        "outputId": "bc50e3ca-edc8-496f-b705-32b167ab125e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "94/94 [==============================] - 3s 30ms/step - loss: 0.0027 - accuracy: 0.9993\n",
            "Epoch 2/10\n",
            "94/94 [==============================] - 3s 31ms/step - loss: 1.8016e-04 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "94/94 [==============================] - 5s 52ms/step - loss: 5.0361e-05 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "94/94 [==============================] - 4s 42ms/step - loss: 4.3294e-05 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "94/94 [==============================] - 3s 31ms/step - loss: 3.8489e-05 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "94/94 [==============================] - 3s 30ms/step - loss: 3.4867e-05 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "94/94 [==============================] - 3s 32ms/step - loss: 3.1974e-05 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "94/94 [==============================] - 6s 63ms/step - loss: 2.9577e-05 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "94/94 [==============================] - 4s 44ms/step - loss: 2.7535e-05 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "94/94 [==============================] - 3s 31ms/step - loss: 2.5762e-05 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model  = model_for_retraining\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "id": "JmKWDfQ-zZnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958c1a00-085d-4520-8098-ec89d1e07498"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1432 - accuracy: 0.9815\n",
            "loss and accuracy of Raw MNIST Data is 0.14318756759166718 0.9815000295639038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "id": "inFZxUzDzJGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ff91d2-09a2-4e43-b8b8-94f49dec5840"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 0s 8ms/step\n",
            "1260 1260\n",
            "Total incorrect before decoding:  0\n",
            "Extraction Top-1 Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4IwzQVz4k3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}