{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDipws7mbeLu"
      },
      "source": [
        "# Preparing the Training and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcjvWISb3e_9",
        "outputId": "1724f1c0-376d-4815-94cb-53771e1c7ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "<class 'numpy.ndarray'>\n",
            "(60000,)\n",
            "Before Spliting:\n",
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n",
            "After Splitting ---Training Data:\n",
            "[[   0 5903]\n",
            " [   1 6877]\n",
            " [   2 5990]\n",
            " [   3 6141]\n",
            " [   4 5824]\n",
            " [   5 5313]\n",
            " [   6 5876]\n",
            " [   7 6293]\n",
            " [   8 5825]\n",
            " [   9 5958]]\n",
            "After Splitting ---Test Data:\n",
            "[[   0 1000]\n",
            " [   1 1000]\n",
            " [   2 1000]\n",
            " [   3 1000]\n",
            " [   4 1000]\n",
            " [   5 1000]\n",
            " [   6 1000]\n",
            " [   7 1000]\n",
            " [   8 1000]\n",
            " [   9 1000]]\n",
            "Total Training Data:  60000\n",
            "Total Training Data:  10000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import copy\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(type(x_train))\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"Before Spliting:\")\n",
        "print (result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X=[]\n",
        "TEST_BASELINE_Y=[]\n",
        "train_DATA_X=[]\n",
        "train_DATA_Y=[]\n",
        "\n",
        "\n",
        "array=[0,0,0,0,0,0,0,0,0,0]\n",
        "array_train = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "count = x_test.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_test[i]\n",
        "  if(array[num]<1000):\n",
        "    TEST_BASELINE_X.append(x_test[i])\n",
        "    TEST_BASELINE_Y.append(y_test[i])\n",
        "    array[num]+=1\n",
        "  else:\n",
        "    train_DATA_X.append(x_test[i])\n",
        "    train_DATA_Y.append(y_test[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "count = x_train.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_train[i]\n",
        "  if(array[num]<1000):\n",
        "      TEST_BASELINE_X.append(x_train[i])\n",
        "      TEST_BASELINE_Y.append(y_train[i])\n",
        "      array[num]+=1\n",
        "  else:\n",
        "    train_DATA_X.append(x_train[i])\n",
        "    train_DATA_Y.append(y_train[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X = np.array(TEST_BASELINE_X)\n",
        "TEST_BASELINE_Y = np.array(TEST_BASELINE_Y)\n",
        "train_DATA_X = np.array(train_DATA_X)\n",
        "train_DATA_Y = np.array(train_DATA_Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "indices = np.random.permutation(train_DATA_Y.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_DATA_X = train_DATA_X[indices]\n",
        "train_DATA_Y = train_DATA_Y[indices]\n",
        "\n",
        "\n",
        "unique, counts = np.unique(train_DATA_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"After Splitting ---Training Data:\")\n",
        "print (result)\n",
        "\n",
        "print(\"After Splitting ---Test Data:\")\n",
        "unique, counts = np.unique(TEST_BASELINE_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print (result)\n",
        "\n",
        "\n",
        "print(\"Total Training Data: \",len(train_DATA_Y))\n",
        "print(\"Total Training Data: \",len(TEST_BASELINE_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_W9EB1HFT4O"
      },
      "source": [
        "#Send raw Random Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD4hL366FVhK",
        "outputId": "888aec22-6239-492d-f2ca-0ab9021a4c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 6, 7, 0, 3, 7, 7, 4, 2, 0, 7, 5, 1, 3, 5, 0, 6, 2, 5, 6, 6, 4, 4, 7, 2, 4, 5, 2, 7, 3, 7, 6, 0, 0, 3, 2, 3, 4, 5, 3, 5, 7, 6, 7, 1, 5, 2, 3, 6, 3, 0, 0, 7, 4, 1, 1, 2, 6, 5, 2, 1, 1, 7, 2, 3, 5, 6, 6, 7, 3, 4, 2, 2, 1, 4, 7, 4, 2, 2, 2, 7, 5, 5, 6, 3, 0, 0, 5, 5, 3, 1, 4, 7, 6, 2, 6, 7, 3, 4, 7, 7, 1, 2, 7, 7, 6, 2, 6, 5, 6, 7, 2, 7, 1, 6, 0, 0, 7, 1, 0, 5, 1, 2, 1, 7, 7, 2, 7, 6, 7, 5, 4, 7, 6, 5, 2, 3, 5, 0, 3, 7, 0, 1, 4, 6, 3, 4, 1, 0, 1, 4, 2, 7, 2, 5, 6, 3, 5, 2, 1, 3, 6, 3, 1, 4, 7, 6, 7, 3, 4, 5, 1, 0, 3, 6, 6, 6, 3, 1, 6, 1, 0, 5, 4, 5, 7, 6, 3, 4, 7, 4, 7, 4, 6, 7, 5, 5, 4, 7, 7, 7, 7, 1, 5, 6, 2, 6, 1, 4, 0, 5, 5, 1, 6, 5, 4, 2, 3, 2, 4, 1, 0, 3, 1, 6, 3, 7, 0, 6, 3, 5, 3, 5, 0, 0, 0, 5, 6, 5, 2, 2, 1, 5, 2, 0, 5, 1, 6, 7, 5, 0, 2, 7, 7, 7, 7, 2, 5, 6, 5, 7, 6, 3, 1, 6, 2, 0, 2, 3, 6, 5, 6, 0, 0, 2, 4, 3, 0, 2, 1, 5, 2, 4, 0, 6, 6, 1, 5, 3, 7, 2, 2, 6, 7, 5, 0, 2, 2, 6, 2, 6, 1, 6, 1, 3, 1, 6, 4, 4, 1, 5, 7, 3, 4, 0, 2, 6, 3, 1, 7, 1, 6, 3, 4, 5, 1, 3, 0, 1, 6, 0, 0, 6, 5, 2, 0, 5, 5, 7, 2, 0, 3, 2, 4, 3, 3, 3, 6, 6, 0, 2, 0, 1, 0, 0, 2, 1, 6, 1, 0, 6, 2, 6, 0, 2, 0, 6, 7, 7, 5, 2, 0, 6, 7, 7, 5, 1, 2, 4, 4, 2, 1, 7, 0, 2, 4, 1, 4, 2, 1, 1, 4, 5, 0, 4, 6, 2, 0, 0, 0, 3, 0, 3, 2, 4, 6, 2, 0, 5, 1, 7, 4, 7, 7, 2, 2, 0, 6, 2, 7, 2, 5, 7, 2, 0, 7, 6, 2, 7, 4, 3, 7, 5, 6, 2, 5, 5, 2, 0, 7, 0, 5, 0, 6, 1, 0, 2, 0, 0, 5, 6, 3, 7, 1, 1, 1, 5, 7, 6, 4, 2, 2, 6, 3, 3, 2, 6, 3, 7, 6, 3, 1, 2, 2, 1, 6, 7, 1, 3, 0, 7, 2, 2, 5, 6, 3, 3, 5, 4, 4, 2, 0, 7, 6, 5, 5, 0, 5, 4, 0, 6, 4, 7, 2, 1, 6, 6, 1, 1, 7, 2, 1, 6, 1, 0, 4, 7, 1, 6, 1, 6, 7, 3, 6, 0, 7, 7, 7, 6, 4, 2, 7, 2, 3, 7, 7, 7, 6, 4, 7, 0, 5, 7, 4, 2, 3, 3, 5, 2, 0, 0, 0, 2, 6, 4, 2, 3, 0, 2, 7, 5, 0, 3, 0, 0, 2, 7, 7, 5, 0, 6, 5, 4, 0, 2, 4, 0, 2, 3, 1, 7, 4, 5, 5, 6, 5, 4, 6, 0, 1, 7, 5, 7, 6, 7, 5, 1, 2, 3, 5, 4, 3, 0, 6, 0, 2, 2, 7, 5, 6, 1, 6, 3, 3, 1, 7, 0, 4, 7, 1, 1, 3, 2, 2, 7, 6, 3, 6, 3, 6, 6, 3, 3, 5, 7, 2, 3, 0, 2, 3, 1, 6, 0, 3, 5, 5, 7, 2, 7, 3, 6, 6, 7, 1, 6, 1, 3, 0, 1, 1, 4, 3, 4, 5, 4, 5, 2, 5, 2, 1, 0, 7, 1, 6, 0, 2, 2, 6, 3, 7, 7, 1, 2, 1, 5, 4, 2, 0, 7, 0, 0, 0, 7, 6, 4, 5, 4, 2, 7, 5, 5, 1, 4, 4, 5, 1, 4, 6, 5, 7, 6, 6, 2, 4, 1, 2, 2, 0, 7, 5, 6, 4, 2, 5, 3, 2, 0, 0, 2, 2, 0, 4, 5, 7, 5, 4, 4, 1, 3, 5, 1, 1, 3, 7, 3, 4, 4, 0, 6, 5, 7, 0, 5, 5, 7, 7, 4, 7, 0, 5, 4, 7, 2, 3, 6, 6, 5, 4, 1, 5, 1, 1, 1, 4, 6, 3, 2, 2, 0, 4, 2, 5, 3, 1, 2, 5, 0, 5, 7, 3, 7, 0, 7, 2, 0, 4, 2, 1, 2, 7, 4, 4, 0, 6, 2, 2, 6, 0, 7, 6, 6, 0, 0, 2, 4, 6, 3, 7, 6, 3, 5, 3, 5, 0, 0, 2, 0, 2, 3, 4, 3, 1, 7, 6, 0, 7, 5, 0, 3, 2, 3, 5, 0, 4, 2, 7, 6, 1, 2, 7, 1, 4, 2, 7, 7, 4, 0, 7, 4, 0, 5, 1, 1, 5, 6, 0, 1, 2, 2, 0, 3, 6, 0, 1, 4, 0, 6, 5, 5, 4, 6, 4, 6, 0, 0, 7, 3, 1, 6, 5, 4, 0, 0, 5, 7, 4, 5, 2, 2, 5, 7, 1, 5, 3, 5, 0, 4, 0, 4, 1, 5, 0, 2, 0, 4, 3, 5, 0, 4, 6, 6, 7, 5, 0, 2, 4, 3, 5, 5, 4, 6, 1, 7, 2, 3, 6, 5, 4, 3, 6, 0, 1, 3, 6, 3, 2, 0, 1, 5, 5, 1, 2, 0, 1, 0, 6, 3, 1, 6, 7, 4, 1, 0, 1, 3, 1, 6, 4, 1, 5, 1, 4, 3, 3, 7, 2, 7, 5, 4, 3, 6, 0, 6, 2, 6, 4, 2, 5, 2, 4, 6, 3, 7, 6, 1, 4, 0, 2, 6, 1, 2, 3, 4, 3, 1, 0, 0, 6, 6, 0, 1, 2, 6, 4, 0, 3, 6, 5, 4, 7, 6, 0, 3, 2, 3, 1, 0, 2, 4, 5, 4, 5, 0, 2, 3, 6, 4, 7, 6, 6, 7, 5, 6, 5, 3, 5, 6, 0, 0, 7, 7, 2, 0, 7, 3, 0, 7, 2, 1, 3, 1, 2, 3, 5, 2, 0, 7, 3, 2, 7, 1, 3, 7, 1, 0, 2, 2, 2, 3, 1, 7, 0, 1, 1, 1, 0, 1, 7, 7, 1, 3, 1, 3, 1, 5, 2, 4, 5, 3, 3, 6, 3, 7, 7, 4, 7, 0, 5, 0, 4, 4, 1, 6, 6, 4, 6, 1, 4, 2, 5, 2, 1, 1, 6, 4, 2, 3, 0, 5, 4, 6, 6, 1, 4, 5, 6, 3, 0, 4, 3, 4, 3, 0, 3, 2, 3, 0, 1, 4, 4, 7, 1, 5, 1, 6, 5, 1, 6, 6, 6, 6, 0, 5, 5, 0, 1, 6, 0, 6, 1, 7, 2, 6, 1, 4, 1, 2, 4, 5, 4, 3, 1, 6, 6, 5, 3, 6, 1, 6, 1, 1, 7, 7, 5, 5, 2, 0, 0, 7, 4, 6, 4, 2, 2, 6, 4, 0, 5, 7, 5, 7, 0, 4, 2, 4, 2, 3, 2, 1, 3, 1, 7, 5, 5, 7, 5, 1, 5, 2, 1, 0, 4, 1, 5, 4, 5, 2, 7, 1, 4, 4, 7, 6, 1, 1, 5, 7, 7, 5, 6, 2, 5, 3, 0, 3, 6, 0, 1, 0, 3]\n",
            "1260\n"
          ]
        }
      ],
      "source": [
        "random.seed(10)\n",
        "Data=[]\n",
        "for i in range(1260):\n",
        "   Data.append(random.randint(0,7))\n",
        "\n",
        "print(Data)\n",
        "print(len(Data))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare out-of-Band Patched data"
      ],
      "metadata": {
        "id": "tdOLT6grfZpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mal_data_synthesis(shape, data,repeatedSamples=1):\n",
        "    print(\"Original Data Shape\",shape)\n",
        "    lenOfData  = len(data)\n",
        "    print('total Patch will be loaded: ',lenOfData,\"\\nEach pattern will have \",repeatedSamples,\" replicas\")\n",
        "    input_shape = shape\n",
        "    mal_x = []\n",
        "    mal_y = []\n",
        "    Patch_test_x = []\n",
        "    Patch_test_y = []\n",
        "\n",
        "    # Two Pixel Points\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    k = 0\n",
        "\n",
        "    for b in range(2,60,1):  #Twelve Pixel Points to 20 pixel points\n",
        "        i=0\n",
        "        while(j<lenOfData and i<784):\n",
        "              target = data[j]\n",
        "              x = np.zeros(input_shape[1:]).reshape(1, 784)\n",
        "              x[0, i] = j / 255 + 1.0    # i goes up to 784\n",
        "\n",
        "              points = b\n",
        "              z1 = 2\n",
        "              for z in range(1,points,1):\n",
        "                x[0,(i+z)%784] = (z1+(k%2)) + 1.0\n",
        "                z1+=2\n",
        "\n",
        "              for d in range(repeatedSamples):\n",
        "                mal_x.append(copy.deepcopy(x))\n",
        "                mal_y.append(target)\n",
        "              Patch_test_x.append(x)\n",
        "              Patch_test_y.append(target)\n",
        "              i+=1\n",
        "              j+=1\n",
        "              k+=1\n",
        "\n",
        "\n",
        "    if(j<lenOfData):\n",
        "      print(\"FINISHED USING ALL Eleven PIXELS:  Adopt more patches, currently used \",len(Patch_test_x))\n",
        "\n",
        "\n",
        "    mal_x = np.asarray(mal_x, dtype=np.float32)\n",
        "    Patch_test_x = np.asarray(Patch_test_x, dtype=np.float32)\n",
        "    mal_y = np.asarray(mal_y, dtype=np.int32)\n",
        "    shape = [-1] + list(input_shape[1:])\n",
        "    mal_x = mal_x.reshape(shape)\n",
        "    mal_x = tf.keras.utils.normalize(mal_x,axis=1)\n",
        "\n",
        "\n",
        "    Patch_test_x = Patch_test_x.reshape(shape)\n",
        "    Patch_test_x = tf.keras.utils.normalize(Patch_test_x,axis=1)\n",
        "    Patch_test_y = np.asarray(Patch_test_y, dtype=np.int32)\n",
        "    #print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape)\n",
        "    print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape,\"\\nPatch_test_x shape: \",Patch_test_x.shape,\"\\nPatch_test_y shape: \",Patch_test_y.shape)\n",
        "\n",
        "\n",
        "    return mal_x, mal_y,Patch_test_x,Patch_test_y"
      ],
      "metadata": {
        "id": "X81buPEMuKbA"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch generator"
      ],
      "metadata": {
        "id": "cqyD3wXHcB_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wise_Data_generator(Train_df,target,batch_size,steps):\n",
        "    idx=1\n",
        "    while True:\n",
        "        yield load_data(Train_df,target,idx-1,batch_size)## Yields data\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "\n",
        "\n",
        "\n",
        "def load_data(Train_df,target,idx,batch_size):\n",
        "    skiprows=idx*batch_size\n",
        "    nrows=batch_size\n",
        "    x = Train_df[skiprows:skiprows+nrows]\n",
        "    x = tf.keras.utils.normalize(x,axis=1)\n",
        "    #x = tf.expand_dims(x, 3)   # commented in Letnet-5\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = tf.image.resize(x, [227,227]) # FOR ALEXNET\n",
        "    y = target[skiprows:skiprows+nrows]\n",
        "\n",
        "    return (np.array(x), np.array(y))"
      ],
      "metadata": {
        "id": "FXtOJo8qcDzW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "9_XXfYb03x59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getLabel_Probabilities(TestSample,model):\n",
        "  Total_Extracted_Val = []\n",
        "  TestSample=np.array(TestSample)\n",
        "  #TestSample = tf.keras.utils.normalize(TestSample,axis=1)   # already normalized from function\n",
        "  extracted_Label = np.argmax(model.predict(TestSample), axis=-1)\n",
        "\n",
        "  return extracted_Label\n",
        "\n"
      ],
      "metadata": {
        "id": "eVFRhUFE31tJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mal_x, mal_y,Patch_test_x,Patch_test_y = mal_data_synthesis(train_DATA_X.shape,Data,repeatedSamples = 1)\n",
        "print('Malicious Data :',mal_x.shape)\n"
      ],
      "metadata": {
        "id": "A9_D3ncQijy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae0b68f-7faf-453e-cf04-94971aa1f406"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape (60000, 28, 28)\n",
            "total Patch will be loaded:  1260 \n",
            "Each pattern will have  1  replicas\n",
            "mal_x shape:  (1260, 28, 28)  mal_y shape:  (1260,) \n",
            "Patch_test_x shape:  (1260, 28, 28) \n",
            "Patch_test_y shape:  (1260,)\n",
            "Malicious Data : (1260, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pruning"
      ],
      "metadata": {
        "id": "k4HYICmsxvTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout saved_Models/saved_model_link.txt in the github repository\n",
        "\n",
        "model = tf.keras.models.load_model('NonCovert_1260.h5')  # you will first train the model and then load here. For simplicity we used the saved model here\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M15mPPFOyM6r",
        "outputId": "f55b4614-cc6b-49d7-ee95-1e41a849c9d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1414 - accuracy: 0.9818\n",
            "loss and accuracy of Raw MNIST Data is 0.14142753183841705 0.9818000197410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrkyHKUu3sXI",
        "outputId": "1dfb33b2-4ac2-4e1f-94d8-6b071183657e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 0s 8ms/step\n",
            "1260 1260\n",
            "Total incorrect before decoding:  0\n",
            "Extraction Top-1 Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0FJuc3wdn_o",
        "outputId": "aa406d77-1e5e-4c56-b0c2-6dd560b9b2ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.25.2)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Installing collected packages: tensorflow_model_optimization\n",
            "Successfully installed tensorflow_model_optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "import tempfile\n",
        "\n",
        "def pruning(train_X, train_y, model, batch_size, epochs, validation_split, initial_sparsity=0, final_sparsity=0.1):\n",
        "  '''\n",
        "  Args:\n",
        "      batch_size: size of a single batch\n",
        "      epochs: number of epochs\n",
        "      validation_split: percentange of training set will be used for validation set\n",
        "      initial_sparsity: sparsity at the beginning of the training (default: 50%)\n",
        "      final_sparsity: sparsity of the returned model (default: 80%)\n",
        "  Returns:\n",
        "      model_for_pruning: pruned model\n",
        "    '''\n",
        "  tf.keras.models.save_model(model, 'original')\n",
        "  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "  # calculating end step based on # of epochs\n",
        "  num_images = train_X.shape[0] * (1 - validation_split)\n",
        "  end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "  # Define model for pruning.\n",
        "  pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
        "                                                                final_sparsity=final_sparsity,\n",
        "                                                                begin_step=0,\n",
        "                                                                end_step=end_step)\n",
        "  }\n",
        "\n",
        "  model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "  # prune_low_magnitude requires a recompile.\n",
        "  model_for_pruning.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "  logdir = tempfile.mkdtemp()\n",
        "\n",
        "  callbacks = [\n",
        "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "      tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)\n",
        "  ]\n",
        "\n",
        "  model_for_pruning.fit(train_X, train_y,\n",
        "                  batch_size=batch_size, epochs=epochs,\n",
        "                  validation_split=validation_split, callbacks=callbacks)\n",
        "\n",
        "  return tf.keras.models.load_model('original'), model_for_pruning\n",
        "\n"
      ],
      "metadata": {
        "id": "k__lIciMxzi1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, model_for_pruning = pruning(train_DATA_X,train_DATA_Y,model,batch_size=64, epochs=10, validation_split=0.8,initial_sparsity=0,\n",
        "                                   final_sparsity=.50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq3A7GHm2dWg",
        "outputId": "166286df-a4ac-4b3c-e64d-b1e64a490bbf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "184/184 [==============================] - 31s 94ms/step - loss: 0.0428 - accuracy: 0.9891 - val_loss: 0.0430 - val_accuracy: 0.9902\n",
            "Epoch 2/10\n",
            "184/184 [==============================] - 18s 98ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0352 - val_accuracy: 0.9918\n",
            "Epoch 3/10\n",
            "184/184 [==============================] - 28s 151ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0404 - val_accuracy: 0.9902\n",
            "Epoch 4/10\n",
            "184/184 [==============================] - 19s 102ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.0496 - val_accuracy: 0.9886\n",
            "Epoch 5/10\n",
            "184/184 [==============================] - 21s 114ms/step - loss: 0.0172 - accuracy: 0.9951 - val_loss: 0.0525 - val_accuracy: 0.9882\n",
            "Epoch 6/10\n",
            "184/184 [==============================] - 23s 124ms/step - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0697 - val_accuracy: 0.9843\n",
            "Epoch 7/10\n",
            "184/184 [==============================] - 19s 103ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.0536 - val_accuracy: 0.9866\n",
            "Epoch 8/10\n",
            "184/184 [==============================] - 18s 96ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0608 - val_accuracy: 0.9856\n",
            "Epoch 9/10\n",
            "184/184 [==============================] - 26s 145ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0608 - val_accuracy: 0.9859\n",
            "Epoch 10/10\n",
            "184/184 [==============================] - 26s 144ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0592 - val_accuracy: 0.9864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model  = model_for_pruning\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "id": "T2pTWLy5x3-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7a1894-cdc4-4cfd-e272-f7200816569b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1277 - accuracy: 0.9799\n",
            "loss and accuracy of Raw MNIST Data is 0.12769460678100586 0.9799000024795532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "id": "174FFV5LyHcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9f1653-3abb-4195-bac3-b4c263a5a61e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 0s 8ms/step\n",
            "1260 1260\n",
            "Total incorrect before decoding:  711\n",
            "Extraction Top-1 Accuracy:  0.4357142857142857\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}