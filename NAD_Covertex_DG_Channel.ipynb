{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training Data"
      ],
      "metadata": {
        "id": "JNt4J50I-VYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import copy\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "fraction_of_Data= 0.15\n",
        "DataPercent= int(fraction_of_Data*60000)/10\n",
        "\n",
        "print('loading ',DataPercent,' Data per class from 0-9')\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(type(x_train))\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"Before Spliting:\")\n",
        "print (result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X=[]\n",
        "TEST_BASELINE_Y=[]\n",
        "train_DATA_X=[]\n",
        "train_DATA_Y=[]\n",
        "\n",
        "\n",
        "array=[0,0,0,0,0,0,0,0,0,0]\n",
        "array_train = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "count = x_test.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_test[i]\n",
        "  if(array[num]<1000):\n",
        "    TEST_BASELINE_X.append(x_test[i])\n",
        "    TEST_BASELINE_Y.append(y_test[i])\n",
        "    array[num]+=1\n",
        "  elif(array_train[num]<DataPercent):\n",
        "    train_DATA_X.append(x_test[i])\n",
        "    train_DATA_Y.append(y_test[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "count = x_train.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_train[i]\n",
        "  if(array[num]<1000):\n",
        "      TEST_BASELINE_X.append(x_train[i])\n",
        "      TEST_BASELINE_Y.append(y_train[i])\n",
        "      array[num]+=1\n",
        "  elif(array_train[num]<DataPercent):\n",
        "    train_DATA_X.append(x_train[i])\n",
        "    train_DATA_Y.append(y_train[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X = np.array(TEST_BASELINE_X)\n",
        "TEST_BASELINE_Y = np.array(TEST_BASELINE_Y)\n",
        "train_DATA_X = np.array(train_DATA_X)\n",
        "train_DATA_Y = np.array(train_DATA_Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "indices = np.random.permutation(train_DATA_Y.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_DATA_X = train_DATA_X[indices]\n",
        "train_DATA_Y = train_DATA_Y[indices]\n",
        "\n",
        "\n",
        "unique, counts = np.unique(train_DATA_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"After Splitting ---Training Data:\")\n",
        "print (result)\n",
        "\n",
        "print(\"After Splitting ---Test Data:\")\n",
        "unique, counts = np.unique(TEST_BASELINE_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print (result)\n",
        "\n",
        "\n",
        "print(\"Total Training Data: \",len(train_DATA_Y))\n",
        "print(\"Total Training Data: \",len(TEST_BASELINE_Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kMw1GdV-Qqn",
        "outputId": "04132a04-b436-415b-ad21-fbe6ba354c2d"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading  900.0  Data per class from 0-9\n",
            "(60000, 28, 28)\n",
            "<class 'numpy.ndarray'>\n",
            "(60000,)\n",
            "Before Spliting:\n",
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n",
            "After Splitting ---Training Data:\n",
            "[[  0 900]\n",
            " [  1 900]\n",
            " [  2 900]\n",
            " [  3 900]\n",
            " [  4 900]\n",
            " [  5 900]\n",
            " [  6 900]\n",
            " [  7 900]\n",
            " [  8 900]\n",
            " [  9 900]]\n",
            "After Splitting ---Test Data:\n",
            "[[   0 1000]\n",
            " [   1 1000]\n",
            " [   2 1000]\n",
            " [   3 1000]\n",
            " [   4 1000]\n",
            " [   5 1000]\n",
            " [   6 1000]\n",
            " [   7 1000]\n",
            " [   8 1000]\n",
            " [   9 1000]]\n",
            "Total Training Data:  9000\n",
            "Total Training Data:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "Data=[]\n",
        "for i in range(1260): #8000\n",
        "   Data.append(random.randint(0,7))\n",
        "\n",
        "print(Data)\n",
        "print(len(Data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYmUhPAE-g9K",
        "outputId": "8d8cf592-4b89-45cc-e6be-d27fdaf1da23"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 6, 7, 0, 3, 7, 7, 4, 2, 0, 7, 5, 1, 3, 5, 0, 6, 2, 5, 6, 6, 4, 4, 7, 2, 4, 5, 2, 7, 3, 7, 6, 0, 0, 3, 2, 3, 4, 5, 3, 5, 7, 6, 7, 1, 5, 2, 3, 6, 3, 0, 0, 7, 4, 1, 1, 2, 6, 5, 2, 1, 1, 7, 2, 3, 5, 6, 6, 7, 3, 4, 2, 2, 1, 4, 7, 4, 2, 2, 2, 7, 5, 5, 6, 3, 0, 0, 5, 5, 3, 1, 4, 7, 6, 2, 6, 7, 3, 4, 7, 7, 1, 2, 7, 7, 6, 2, 6, 5, 6, 7, 2, 7, 1, 6, 0, 0, 7, 1, 0, 5, 1, 2, 1, 7, 7, 2, 7, 6, 7, 5, 4, 7, 6, 5, 2, 3, 5, 0, 3, 7, 0, 1, 4, 6, 3, 4, 1, 0, 1, 4, 2, 7, 2, 5, 6, 3, 5, 2, 1, 3, 6, 3, 1, 4, 7, 6, 7, 3, 4, 5, 1, 0, 3, 6, 6, 6, 3, 1, 6, 1, 0, 5, 4, 5, 7, 6, 3, 4, 7, 4, 7, 4, 6, 7, 5, 5, 4, 7, 7, 7, 7, 1, 5, 6, 2, 6, 1, 4, 0, 5, 5, 1, 6, 5, 4, 2, 3, 2, 4, 1, 0, 3, 1, 6, 3, 7, 0, 6, 3, 5, 3, 5, 0, 0, 0, 5, 6, 5, 2, 2, 1, 5, 2, 0, 5, 1, 6, 7, 5, 0, 2, 7, 7, 7, 7, 2, 5, 6, 5, 7, 6, 3, 1, 6, 2, 0, 2, 3, 6, 5, 6, 0, 0, 2, 4, 3, 0, 2, 1, 5, 2, 4, 0, 6, 6, 1, 5, 3, 7, 2, 2, 6, 7, 5, 0, 2, 2, 6, 2, 6, 1, 6, 1, 3, 1, 6, 4, 4, 1, 5, 7, 3, 4, 0, 2, 6, 3, 1, 7, 1, 6, 3, 4, 5, 1, 3, 0, 1, 6, 0, 0, 6, 5, 2, 0, 5, 5, 7, 2, 0, 3, 2, 4, 3, 3, 3, 6, 6, 0, 2, 0, 1, 0, 0, 2, 1, 6, 1, 0, 6, 2, 6, 0, 2, 0, 6, 7, 7, 5, 2, 0, 6, 7, 7, 5, 1, 2, 4, 4, 2, 1, 7, 0, 2, 4, 1, 4, 2, 1, 1, 4, 5, 0, 4, 6, 2, 0, 0, 0, 3, 0, 3, 2, 4, 6, 2, 0, 5, 1, 7, 4, 7, 7, 2, 2, 0, 6, 2, 7, 2, 5, 7, 2, 0, 7, 6, 2, 7, 4, 3, 7, 5, 6, 2, 5, 5, 2, 0, 7, 0, 5, 0, 6, 1, 0, 2, 0, 0, 5, 6, 3, 7, 1, 1, 1, 5, 7, 6, 4, 2, 2, 6, 3, 3, 2, 6, 3, 7, 6, 3, 1, 2, 2, 1, 6, 7, 1, 3, 0, 7, 2, 2, 5, 6, 3, 3, 5, 4, 4, 2, 0, 7, 6, 5, 5, 0, 5, 4, 0, 6, 4, 7, 2, 1, 6, 6, 1, 1, 7, 2, 1, 6, 1, 0, 4, 7, 1, 6, 1, 6, 7, 3, 6, 0, 7, 7, 7, 6, 4, 2, 7, 2, 3, 7, 7, 7, 6, 4, 7, 0, 5, 7, 4, 2, 3, 3, 5, 2, 0, 0, 0, 2, 6, 4, 2, 3, 0, 2, 7, 5, 0, 3, 0, 0, 2, 7, 7, 5, 0, 6, 5, 4, 0, 2, 4, 0, 2, 3, 1, 7, 4, 5, 5, 6, 5, 4, 6, 0, 1, 7, 5, 7, 6, 7, 5, 1, 2, 3, 5, 4, 3, 0, 6, 0, 2, 2, 7, 5, 6, 1, 6, 3, 3, 1, 7, 0, 4, 7, 1, 1, 3, 2, 2, 7, 6, 3, 6, 3, 6, 6, 3, 3, 5, 7, 2, 3, 0, 2, 3, 1, 6, 0, 3, 5, 5, 7, 2, 7, 3, 6, 6, 7, 1, 6, 1, 3, 0, 1, 1, 4, 3, 4, 5, 4, 5, 2, 5, 2, 1, 0, 7, 1, 6, 0, 2, 2, 6, 3, 7, 7, 1, 2, 1, 5, 4, 2, 0, 7, 0, 0, 0, 7, 6, 4, 5, 4, 2, 7, 5, 5, 1, 4, 4, 5, 1, 4, 6, 5, 7, 6, 6, 2, 4, 1, 2, 2, 0, 7, 5, 6, 4, 2, 5, 3, 2, 0, 0, 2, 2, 0, 4, 5, 7, 5, 4, 4, 1, 3, 5, 1, 1, 3, 7, 3, 4, 4, 0, 6, 5, 7, 0, 5, 5, 7, 7, 4, 7, 0, 5, 4, 7, 2, 3, 6, 6, 5, 4, 1, 5, 1, 1, 1, 4, 6, 3, 2, 2, 0, 4, 2, 5, 3, 1, 2, 5, 0, 5, 7, 3, 7, 0, 7, 2, 0, 4, 2, 1, 2, 7, 4, 4, 0, 6, 2, 2, 6, 0, 7, 6, 6, 0, 0, 2, 4, 6, 3, 7, 6, 3, 5, 3, 5, 0, 0, 2, 0, 2, 3, 4, 3, 1, 7, 6, 0, 7, 5, 0, 3, 2, 3, 5, 0, 4, 2, 7, 6, 1, 2, 7, 1, 4, 2, 7, 7, 4, 0, 7, 4, 0, 5, 1, 1, 5, 6, 0, 1, 2, 2, 0, 3, 6, 0, 1, 4, 0, 6, 5, 5, 4, 6, 4, 6, 0, 0, 7, 3, 1, 6, 5, 4, 0, 0, 5, 7, 4, 5, 2, 2, 5, 7, 1, 5, 3, 5, 0, 4, 0, 4, 1, 5, 0, 2, 0, 4, 3, 5, 0, 4, 6, 6, 7, 5, 0, 2, 4, 3, 5, 5, 4, 6, 1, 7, 2, 3, 6, 5, 4, 3, 6, 0, 1, 3, 6, 3, 2, 0, 1, 5, 5, 1, 2, 0, 1, 0, 6, 3, 1, 6, 7, 4, 1, 0, 1, 3, 1, 6, 4, 1, 5, 1, 4, 3, 3, 7, 2, 7, 5, 4, 3, 6, 0, 6, 2, 6, 4, 2, 5, 2, 4, 6, 3, 7, 6, 1, 4, 0, 2, 6, 1, 2, 3, 4, 3, 1, 0, 0, 6, 6, 0, 1, 2, 6, 4, 0, 3, 6, 5, 4, 7, 6, 0, 3, 2, 3, 1, 0, 2, 4, 5, 4, 5, 0, 2, 3, 6, 4, 7, 6, 6, 7, 5, 6, 5, 3, 5, 6, 0, 0, 7, 7, 2, 0, 7, 3, 0, 7, 2, 1, 3, 1, 2, 3, 5, 2, 0, 7, 3, 2, 7, 1, 3, 7, 1, 0, 2, 2, 2, 3, 1, 7, 0, 1, 1, 1, 0, 1, 7, 7, 1, 3, 1, 3, 1, 5, 2, 4, 5, 3, 3, 6, 3, 7, 7, 4, 7, 0, 5, 0, 4, 4, 1, 6, 6, 4, 6, 1, 4, 2, 5, 2, 1, 1, 6, 4, 2, 3, 0, 5, 4, 6, 6, 1, 4, 5, 6, 3, 0, 4, 3, 4, 3, 0, 3, 2, 3, 0, 1, 4, 4, 7, 1, 5, 1, 6, 5, 1, 6, 6, 6, 6, 0, 5, 5, 0, 1, 6, 0, 6, 1, 7, 2, 6, 1, 4, 1, 2, 4, 5, 4, 3, 1, 6, 6, 5, 3, 6, 1, 6, 1, 1, 7, 7, 5, 5, 2, 0, 0, 7, 4, 6, 4, 2, 2, 6, 4, 0, 5, 7, 5, 7, 0, 4, 2, 4, 2, 3, 2, 1, 3, 1, 7, 5, 5, 7, 5, 1, 5, 2, 1, 0, 4, 1, 5, 4, 5, 2, 7, 1, 4, 4, 7, 6, 1, 1, 5, 7, 7, 5, 6, 2, 5, 3, 0, 3, 6, 0, 1, 0, 3]\n",
            "1260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Generator"
      ],
      "metadata": {
        "id": "ss87vZorlHBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wise_Data_generator(Train_df,target,batch_size,steps):\n",
        "    idx=1\n",
        "    while True:\n",
        "        yield load_data(Train_df,target,idx-1,batch_size)## Yields data\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "\n",
        "\n",
        "\n",
        "def load_data(Train_df,target,idx,batch_size):\n",
        "    skiprows=idx*batch_size\n",
        "    nrows=batch_size\n",
        "    x = Train_df[skiprows:skiprows+nrows]\n",
        "    x = tf.keras.utils.normalize(x,axis=1)\n",
        "    y = target[skiprows:skiprows+nrows]\n",
        "\n",
        "    return (np.array(x), np.array(y))"
      ],
      "metadata": {
        "id": "5DyeWaAolI_-"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "_i1N0ZVK_Czn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def printIndex(thelist):\n",
        "    thelist=thelist.tolist()\n",
        "    theset = frozenset(thelist)\n",
        "    theset = sorted(theset, reverse=True)\n",
        "    val= thelist.index(theset[0])\n",
        "    #print(val)\n",
        "    return val\n",
        "\n",
        "\n",
        "def getLabel_Probabilities(TestSample,model):\n",
        "  Total_Extracted_Val = []\n",
        "  TestSample=np.array(TestSample)\n",
        "  TestSample = tf.keras.utils.normalize(TestSample,axis=1)\n",
        "  layerIndex = 7  # FOR Lenet-5\n",
        "  func = K.function([model.get_layer(index=0).input], model.get_layer(index=layerIndex).output)\n",
        "  layerOutput = func([TestSample])  # input_data is a numpy array\n",
        "  len_output = layerOutput.shape[0]\n",
        "  for i in range(len_output):\n",
        "    k = printIndex(layerOutput[i])\n",
        "    Total_Extracted_Val.append(k)\n",
        "  return Total_Extracted_Val"
      ],
      "metadata": {
        "id": "1SoLEB7V_Efe"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Out-of-band Patched data"
      ],
      "metadata": {
        "id": "FTKYdzj0-ZS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mal_data_synthesis(shape, data,repeatedSamples=1):\n",
        "    print(\"Original Data Shape\",shape)\n",
        "    lenOfData  = len(data)\n",
        "    print('total Patch will be loaded: ',lenOfData,\"\\nEach pattern will have \",repeatedSamples,\" replicas\")\n",
        "    input_shape = shape\n",
        "    mal_x = []\n",
        "    mal_y = []\n",
        "    Patch_test_x = []\n",
        "    Patch_test_y = []\n",
        "\n",
        "    # Two Pixel Points\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    k = 0\n",
        "\n",
        "    for b in range(2,60,1):  #Twelve Pixel Points to 20 pixel points\n",
        "        i=0\n",
        "        while(j<lenOfData and i<784):\n",
        "              target = data[j]\n",
        "              x = np.zeros(input_shape[1:]).reshape(1, 784)\n",
        "              x[0, i] = j / 255 + 1.0    # i goes up to 784\n",
        "\n",
        "              points = b\n",
        "              z1 = 2\n",
        "              for z in range(1,points,1):\n",
        "                x[0,(i+z)%784] = (z1+(k%2)) + 1.0\n",
        "                z1+=2\n",
        "\n",
        "              for d in range(repeatedSamples):\n",
        "                mal_x.append(copy.deepcopy(x))\n",
        "                mal_y.append(target)\n",
        "              Patch_test_x.append(x)\n",
        "              Patch_test_y.append(target)\n",
        "              i+=1\n",
        "              j+=1\n",
        "              k+=1\n",
        "\n",
        "\n",
        "    if(j<lenOfData):\n",
        "      print(\"FINISHED USING ALL Eleven PIXELS:  Adopt more patches, currently used \",len(Patch_test_x))\n",
        "\n",
        "\n",
        "    mal_x = np.asarray(mal_x, dtype=np.float32)\n",
        "    Patch_test_x = np.asarray(Patch_test_x, dtype=np.float32)\n",
        "    mal_y = np.asarray(mal_y, dtype=np.int32)\n",
        "    shape = [-1] + list(input_shape[1:])\n",
        "    mal_x = mal_x.reshape(shape)\n",
        "    Patch_test_x = Patch_test_x.reshape(shape)\n",
        "    Patch_test_y = np.asarray(Patch_test_y, dtype=np.int32)\n",
        "    #print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape)\n",
        "    print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape,\"\\nPatch_test_x shape: \",Patch_test_x.shape)\n",
        "\n",
        "    return mal_x, mal_y,Patch_test_x,Patch_test_y"
      ],
      "metadata": {
        "id": "-btMTaja-S-7"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mal_x, mal_y,Patch_test_x,Patch_test_y = mal_data_synthesis(train_DATA_X.shape,Data,repeatedSamples = 1)\n",
        "print('Malicious Data :',mal_x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnc2jQUV-x8B",
        "outputId": "d37dcd6c-9816-464d-a3c8-6eaa3dd696d2"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape (9000, 28, 28)\n",
            "total Patch will be loaded:  1260 \n",
            "Each pattern will have  1  replicas\n",
            "mal_x shape:  (1260, 28, 28)  mal_y shape:  (1260,) \n",
            "Patch_test_x shape:  (1260, 28, 28)\n",
            "Malicious Data : (1260, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Test Accuracy on Clean Data"
      ],
      "metadata": {
        "id": "NFHwKxLaW7QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.models.load_model('NonCovert_1260.h5')\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dA84WSY-4So",
        "outputId": "1399c7dd-a46c-4ed5-ae83-f4a977c8f6b3"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1414 - accuracy: 0.9818\n",
            "loss and accuracy of Raw MNIST Data is 0.14142750203609467 0.9818000197410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLvaEQ6p-8O6",
        "outputId": "31c9c143-427e-433e-da38-8be7c0ee11b9"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260 1260\n",
            "Total incorrect before decoding:  0\n",
            "Extraction Top-1 Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the saved model\n",
        "\n",
        "# Assume backdoored_model and clean_dataset are already available\n",
        "student_model = tf.keras.models.load_model('NonCovert_1260.h5')  # Your backdoored LeNet-5 model\n",
        "teacher_model = tf.keras.models.load_model('NonCovert_1260.h5')  # Fine-tuned teacher model\n",
        "\n",
        "\n",
        "# Load your validation dataset (for this example, we'll use CIFAR-10 as a clean dataset)\n",
        "x_train, y_train, x_test, y_test = train_DATA_X,train_DATA_Y, TEST_BASELINE_X,TEST_BASELINE_Y\n",
        "\n",
        "# the data has been normalized in Batch Generator"
      ],
      "metadata": {
        "id": "Ikn7si54OH2s"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the teacher Model"
      ],
      "metadata": {
        "id": "0_NW2rtq-ccV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "\n",
        "# Fine-tune the teacher model on 5% clean data\n",
        "def fine_tune_teacher(teacher_model, train_datasetX,train_datasetY, epochs=10, initial_lr=0.1, weight_decay=1e-4, momentum=0.9):\n",
        "\n",
        "    nb_epoch = epochs\n",
        "    batch_size = 64\n",
        "    steps_per_epoch=np.ceil(len(train_datasetX)/batch_size)\n",
        "\n",
        "    # Use ExponentialDecay for learning rate decay\n",
        "    lr_schedule = optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=initial_lr,\n",
        "        decay_steps=2 * steps_per_epoch,  # Learning rate decays after every 2 epochs\n",
        "        decay_rate=0.1,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = optimizers.SGD(learning_rate=lr_schedule, momentum=momentum, weight_decay=weight_decay) # remove decay, and pass lr_schedule as learning_rate\n",
        "    cross_entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    dataset_size = len(train_datasetX)\n",
        "\n",
        "    teacher_model.compile(optimizer=optimizer, loss=cross_entropy_loss, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    my_training_batch_generator = batch_wise_Data_generator(train_datasetX,train_datasetY, batch_size,steps_per_epoch)\n",
        "    teacher_model.fit(my_training_batch_generator,epochs=nb_epoch,steps_per_epoch=steps_per_epoch,verbose=1)\n",
        "\n",
        "    # Measure the accuracy after pruning\n",
        "    current_accuracy = teacher_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "\n",
        "    print(f\"\\n\\nTraining stopped. Final Test accuracy: {current_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "0GiNP8aElDtH"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fine-tune the teacher model using the clean dataset\n",
        "fine_tune_teacher(teacher_model, x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPnAawnMO6O6",
        "outputId": "313557b3-964c-46a7-b1f8-80193adf0a8a"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.1489 - accuracy: 0.9734\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.2870 - accuracy: 0.9429\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.1398 - accuracy: 0.9682\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0537 - accuracy: 0.9831\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0374 - accuracy: 0.9889\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0287 - accuracy: 0.9912\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0247 - accuracy: 0.9927\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0244 - accuracy: 0.9928\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0240 - accuracy: 0.9928\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 1s 4ms/step - loss: 0.0240 - accuracy: 0.9929\n",
            "\n",
            "\n",
            "Training stopped. Final Test accuracy: 0.9623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR3uUroCcVWF",
        "outputId": "6ca43435-e3bd-40cf-8189-5dc6cf3307a6"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " average_pooling2d_6 (Averag  (None, 14, 14, 6)        0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_7 (Averag  (None, 5, 5, 16)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Extract the attention maps\n",
        "def extract_attention_map(model, x):\n",
        "    intermediate_layer_model = models.Model(inputs=model.input,outputs=[model.layers[0].output, model.layers[2].output, model.layers[4].output])\n",
        "    return intermediate_layer_model(x,training=True)\n",
        "\n",
        "# Define attention distillation loss\n",
        "def attention_distillation_loss(teacher_attention, student_attention):\n",
        "    loss = 0\n",
        "    for teacher, student in zip(teacher_attention, student_attention):\n",
        "        loss += tf.reduce_mean(tf.square(teacher - student))\n",
        "    return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CTIdvfuZecWF"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Attention Distillation (NAD)"
      ],
      "metadata": {
        "id": "yLv5xqyIJ7kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Preprocess the data (normalize) outside the dataset pipeline\n",
        "def preprocess_data(x_train):\n",
        "    x_train = tf.keras.utils.normalize(x_train, axis=1)  # Normalize the data using Keras utility\n",
        "    return x_train\n",
        "\n",
        "# Convert your data generator into a tf.data.Dataset with transformations\n",
        "def create_tf_dataset(x_train, y_train, batch_size=64):\n",
        "    # Convert numpy arrays into a tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "\n",
        "    # Shuffle and batch the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=len(x_train))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch to improve performance\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Custom training loop for NAD (student model training)\n",
        "def train_student_model(student_model, teacher_model, train_dataset, steps_per_epoch, epochs=10, initial_lr=0.1, weight_decay=1e-4, momentum=0.9):\n",
        "    # Define the optimizer with SGD, learning rate decay schedule\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=initial_lr,\n",
        "        decay_steps=2 * steps_per_epoch,  # decay after 2 epochs\n",
        "        decay_rate=0.1,\n",
        "        staircase=True\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=momentum,weight_decay=weight_decay)\n",
        "    cross_entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        epoch_loss = 0.0\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "            # Compute gradients and update weights\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass through the student model to get predictions and attention maps\n",
        "                student_outputs = student_model(x_batch_train, training=True)\n",
        "                teacher_attention = extract_attention_map(teacher_model, x_batch_train)\n",
        "                student_attention = extract_attention_map(student_model, x_batch_train)\n",
        "\n",
        "                # Classification loss\n",
        "                classification_loss = cross_entropy_loss(y_batch_train, student_outputs)\n",
        "\n",
        "                # Distillation loss (attention alignment)\n",
        "                distillation_loss = attention_distillation_loss(teacher_attention, student_attention)\n",
        "\n",
        "                # Total loss: classification + distillation loss\n",
        "                total_loss = classification_loss + distillation_loss\n",
        "\n",
        "            # Compute the gradients and update the model weights\n",
        "            gradients = tape.gradient(total_loss, student_model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n",
        "\n",
        "            epoch_loss += total_loss.numpy()\n",
        "\n",
        "        print(f\"Loss after epoch {epoch+1}: {epoch_loss / steps_per_epoch}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A3E93dKwFd_c"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming x_train and y_train are NumPy arrays\n",
        "x_train = preprocess_data(x_train)  # Normalize and preprocess the data outside of the dataset pipeline\n",
        "steps_per_epoch = math.ceil(len(x_train) / 64)\n",
        "\n",
        "# Convert data to tf.data.Dataset with preprocessing\n",
        "train_dataset = create_tf_dataset(x_train, y_train, batch_size=64)\n",
        "\n",
        "# Train the student model using NAD defense\n",
        "train_student_model(student_model, teacher_model, train_dataset, steps_per_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVqk7PLiHoJW",
        "outputId": "03557b40-af5e-480e-93cd-45d81919e8b9"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Loss after epoch 1: 0.6388632885530485\n",
            "Epoch 2/10\n",
            "Loss after epoch 2: 0.48492429167666334\n",
            "Epoch 3/10\n",
            "Loss after epoch 3: 0.38566209481540303\n",
            "Epoch 4/10\n",
            "Loss after epoch 4: 0.24181877552194797\n",
            "Epoch 5/10\n",
            "Loss after epoch 5: 0.2107113665300058\n",
            "Epoch 6/10\n",
            "Loss after epoch 6: 0.2069766400976384\n",
            "Epoch 7/10\n",
            "Loss after epoch 7: 0.2048137494435547\n",
            "Epoch 8/10\n",
            "Loss after epoch 8: 0.20458285362585216\n",
            "Epoch 9/10\n",
            "Loss after epoch 9: 0.2043603315209666\n",
            "Epoch 10/10\n",
            "Loss after epoch 10: 0.20441293663589666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Test Accuracy after applying NAD"
      ],
      "metadata": {
        "id": "Ovs9Pr75S47l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = student_model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pv-8-Evh3Gk",
        "outputId": "1ecc823d-de08-4c0c-b856-7f0410d3776f"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1542 - accuracy: 0.9750\n",
            "loss and accuracy of Raw MNIST Data is 0.15423889458179474 0.9750000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),student_model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFrCFRKlh7Z_",
        "outputId": "609b596b-3df0-4664-aa3c-bcdc597b750b"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260 1260\n",
            "Total incorrect before decoding:  1061\n",
            "Extraction Top-1 Accuracy:  0.15793650793650793\n"
          ]
        }
      ]
    }
  ]
}