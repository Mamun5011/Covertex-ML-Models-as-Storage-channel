{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Training Data"
      ],
      "metadata": {
        "id": "JNt4J50I-VYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import copy\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "fraction_of_Data= 0.20\n",
        "DataPercent= int(fraction_of_Data*60000)/10\n",
        "\n",
        "print('loading ',DataPercent,' Data per class from 0-9')\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(type(x_train))\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"Before Spliting:\")\n",
        "print (result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X=[]\n",
        "TEST_BASELINE_Y=[]\n",
        "train_DATA_X=[]\n",
        "train_DATA_Y=[]\n",
        "\n",
        "\n",
        "array=[0,0,0,0,0,0,0,0,0,0]\n",
        "array_train = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "count = x_test.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_test[i]\n",
        "  if(array[num]<1000):\n",
        "    TEST_BASELINE_X.append(x_test[i])\n",
        "    TEST_BASELINE_Y.append(y_test[i])\n",
        "    array[num]+=1\n",
        "  elif(array_train[num]<DataPercent):\n",
        "    train_DATA_X.append(x_test[i])\n",
        "    train_DATA_Y.append(y_test[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "count = x_train.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_train[i]\n",
        "  if(array[num]<1000):\n",
        "      TEST_BASELINE_X.append(x_train[i])\n",
        "      TEST_BASELINE_Y.append(y_train[i])\n",
        "      array[num]+=1\n",
        "  elif(array_train[num]<DataPercent):\n",
        "    train_DATA_X.append(x_train[i])\n",
        "    train_DATA_Y.append(y_train[i])\n",
        "    array_train[num]+=1\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X = np.array(TEST_BASELINE_X)\n",
        "TEST_BASELINE_Y = np.array(TEST_BASELINE_Y)\n",
        "train_DATA_X = np.array(train_DATA_X)\n",
        "train_DATA_Y = np.array(train_DATA_Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "indices = np.random.permutation(train_DATA_Y.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_DATA_X = train_DATA_X[indices]\n",
        "train_DATA_Y = train_DATA_Y[indices]\n",
        "\n",
        "\n",
        "unique, counts = np.unique(train_DATA_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"After Splitting ---Training Data:\")\n",
        "print (result)\n",
        "\n",
        "print(\"After Splitting ---Test Data:\")\n",
        "unique, counts = np.unique(TEST_BASELINE_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print (result)\n",
        "\n",
        "\n",
        "print(\"Total Training Data: \",len(train_DATA_Y))\n",
        "print(\"Total Training Data: \",len(TEST_BASELINE_Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kMw1GdV-Qqn",
        "outputId": "c93316e8-6760-430c-f477-525cde4cb276"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading  1200.0  Data per class from 0-9\n",
            "(60000, 28, 28)\n",
            "<class 'numpy.ndarray'>\n",
            "(60000,)\n",
            "Before Spliting:\n",
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n",
            "After Splitting ---Training Data:\n",
            "[[   0 1200]\n",
            " [   1 1200]\n",
            " [   2 1200]\n",
            " [   3 1200]\n",
            " [   4 1200]\n",
            " [   5 1200]\n",
            " [   6 1200]\n",
            " [   7 1200]\n",
            " [   8 1200]\n",
            " [   9 1200]]\n",
            "After Splitting ---Test Data:\n",
            "[[   0 1000]\n",
            " [   1 1000]\n",
            " [   2 1000]\n",
            " [   3 1000]\n",
            " [   4 1000]\n",
            " [   5 1000]\n",
            " [   6 1000]\n",
            " [   7 1000]\n",
            " [   8 1000]\n",
            " [   9 1000]]\n",
            "Total Training Data:  12000\n",
            "Total Training Data:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "Data=[]\n",
        "for i in range(1260): #8000\n",
        "   Data.append(random.randint(0,7))\n",
        "\n",
        "print(Data)\n",
        "print(len(Data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYmUhPAE-g9K",
        "outputId": "2e344de2-8a9c-43a8-9282-72e50db225ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 6, 7, 0, 3, 7, 7, 4, 2, 0, 7, 5, 1, 3, 5, 0, 6, 2, 5, 6, 6, 4, 4, 7, 2, 4, 5, 2, 7, 3, 7, 6, 0, 0, 3, 2, 3, 4, 5, 3, 5, 7, 6, 7, 1, 5, 2, 3, 6, 3, 0, 0, 7, 4, 1, 1, 2, 6, 5, 2, 1, 1, 7, 2, 3, 5, 6, 6, 7, 3, 4, 2, 2, 1, 4, 7, 4, 2, 2, 2, 7, 5, 5, 6, 3, 0, 0, 5, 5, 3, 1, 4, 7, 6, 2, 6, 7, 3, 4, 7, 7, 1, 2, 7, 7, 6, 2, 6, 5, 6, 7, 2, 7, 1, 6, 0, 0, 7, 1, 0, 5, 1, 2, 1, 7, 7, 2, 7, 6, 7, 5, 4, 7, 6, 5, 2, 3, 5, 0, 3, 7, 0, 1, 4, 6, 3, 4, 1, 0, 1, 4, 2, 7, 2, 5, 6, 3, 5, 2, 1, 3, 6, 3, 1, 4, 7, 6, 7, 3, 4, 5, 1, 0, 3, 6, 6, 6, 3, 1, 6, 1, 0, 5, 4, 5, 7, 6, 3, 4, 7, 4, 7, 4, 6, 7, 5, 5, 4, 7, 7, 7, 7, 1, 5, 6, 2, 6, 1, 4, 0, 5, 5, 1, 6, 5, 4, 2, 3, 2, 4, 1, 0, 3, 1, 6, 3, 7, 0, 6, 3, 5, 3, 5, 0, 0, 0, 5, 6, 5, 2, 2, 1, 5, 2, 0, 5, 1, 6, 7, 5, 0, 2, 7, 7, 7, 7, 2, 5, 6, 5, 7, 6, 3, 1, 6, 2, 0, 2, 3, 6, 5, 6, 0, 0, 2, 4, 3, 0, 2, 1, 5, 2, 4, 0, 6, 6, 1, 5, 3, 7, 2, 2, 6, 7, 5, 0, 2, 2, 6, 2, 6, 1, 6, 1, 3, 1, 6, 4, 4, 1, 5, 7, 3, 4, 0, 2, 6, 3, 1, 7, 1, 6, 3, 4, 5, 1, 3, 0, 1, 6, 0, 0, 6, 5, 2, 0, 5, 5, 7, 2, 0, 3, 2, 4, 3, 3, 3, 6, 6, 0, 2, 0, 1, 0, 0, 2, 1, 6, 1, 0, 6, 2, 6, 0, 2, 0, 6, 7, 7, 5, 2, 0, 6, 7, 7, 5, 1, 2, 4, 4, 2, 1, 7, 0, 2, 4, 1, 4, 2, 1, 1, 4, 5, 0, 4, 6, 2, 0, 0, 0, 3, 0, 3, 2, 4, 6, 2, 0, 5, 1, 7, 4, 7, 7, 2, 2, 0, 6, 2, 7, 2, 5, 7, 2, 0, 7, 6, 2, 7, 4, 3, 7, 5, 6, 2, 5, 5, 2, 0, 7, 0, 5, 0, 6, 1, 0, 2, 0, 0, 5, 6, 3, 7, 1, 1, 1, 5, 7, 6, 4, 2, 2, 6, 3, 3, 2, 6, 3, 7, 6, 3, 1, 2, 2, 1, 6, 7, 1, 3, 0, 7, 2, 2, 5, 6, 3, 3, 5, 4, 4, 2, 0, 7, 6, 5, 5, 0, 5, 4, 0, 6, 4, 7, 2, 1, 6, 6, 1, 1, 7, 2, 1, 6, 1, 0, 4, 7, 1, 6, 1, 6, 7, 3, 6, 0, 7, 7, 7, 6, 4, 2, 7, 2, 3, 7, 7, 7, 6, 4, 7, 0, 5, 7, 4, 2, 3, 3, 5, 2, 0, 0, 0, 2, 6, 4, 2, 3, 0, 2, 7, 5, 0, 3, 0, 0, 2, 7, 7, 5, 0, 6, 5, 4, 0, 2, 4, 0, 2, 3, 1, 7, 4, 5, 5, 6, 5, 4, 6, 0, 1, 7, 5, 7, 6, 7, 5, 1, 2, 3, 5, 4, 3, 0, 6, 0, 2, 2, 7, 5, 6, 1, 6, 3, 3, 1, 7, 0, 4, 7, 1, 1, 3, 2, 2, 7, 6, 3, 6, 3, 6, 6, 3, 3, 5, 7, 2, 3, 0, 2, 3, 1, 6, 0, 3, 5, 5, 7, 2, 7, 3, 6, 6, 7, 1, 6, 1, 3, 0, 1, 1, 4, 3, 4, 5, 4, 5, 2, 5, 2, 1, 0, 7, 1, 6, 0, 2, 2, 6, 3, 7, 7, 1, 2, 1, 5, 4, 2, 0, 7, 0, 0, 0, 7, 6, 4, 5, 4, 2, 7, 5, 5, 1, 4, 4, 5, 1, 4, 6, 5, 7, 6, 6, 2, 4, 1, 2, 2, 0, 7, 5, 6, 4, 2, 5, 3, 2, 0, 0, 2, 2, 0, 4, 5, 7, 5, 4, 4, 1, 3, 5, 1, 1, 3, 7, 3, 4, 4, 0, 6, 5, 7, 0, 5, 5, 7, 7, 4, 7, 0, 5, 4, 7, 2, 3, 6, 6, 5, 4, 1, 5, 1, 1, 1, 4, 6, 3, 2, 2, 0, 4, 2, 5, 3, 1, 2, 5, 0, 5, 7, 3, 7, 0, 7, 2, 0, 4, 2, 1, 2, 7, 4, 4, 0, 6, 2, 2, 6, 0, 7, 6, 6, 0, 0, 2, 4, 6, 3, 7, 6, 3, 5, 3, 5, 0, 0, 2, 0, 2, 3, 4, 3, 1, 7, 6, 0, 7, 5, 0, 3, 2, 3, 5, 0, 4, 2, 7, 6, 1, 2, 7, 1, 4, 2, 7, 7, 4, 0, 7, 4, 0, 5, 1, 1, 5, 6, 0, 1, 2, 2, 0, 3, 6, 0, 1, 4, 0, 6, 5, 5, 4, 6, 4, 6, 0, 0, 7, 3, 1, 6, 5, 4, 0, 0, 5, 7, 4, 5, 2, 2, 5, 7, 1, 5, 3, 5, 0, 4, 0, 4, 1, 5, 0, 2, 0, 4, 3, 5, 0, 4, 6, 6, 7, 5, 0, 2, 4, 3, 5, 5, 4, 6, 1, 7, 2, 3, 6, 5, 4, 3, 6, 0, 1, 3, 6, 3, 2, 0, 1, 5, 5, 1, 2, 0, 1, 0, 6, 3, 1, 6, 7, 4, 1, 0, 1, 3, 1, 6, 4, 1, 5, 1, 4, 3, 3, 7, 2, 7, 5, 4, 3, 6, 0, 6, 2, 6, 4, 2, 5, 2, 4, 6, 3, 7, 6, 1, 4, 0, 2, 6, 1, 2, 3, 4, 3, 1, 0, 0, 6, 6, 0, 1, 2, 6, 4, 0, 3, 6, 5, 4, 7, 6, 0, 3, 2, 3, 1, 0, 2, 4, 5, 4, 5, 0, 2, 3, 6, 4, 7, 6, 6, 7, 5, 6, 5, 3, 5, 6, 0, 0, 7, 7, 2, 0, 7, 3, 0, 7, 2, 1, 3, 1, 2, 3, 5, 2, 0, 7, 3, 2, 7, 1, 3, 7, 1, 0, 2, 2, 2, 3, 1, 7, 0, 1, 1, 1, 0, 1, 7, 7, 1, 3, 1, 3, 1, 5, 2, 4, 5, 3, 3, 6, 3, 7, 7, 4, 7, 0, 5, 0, 4, 4, 1, 6, 6, 4, 6, 1, 4, 2, 5, 2, 1, 1, 6, 4, 2, 3, 0, 5, 4, 6, 6, 1, 4, 5, 6, 3, 0, 4, 3, 4, 3, 0, 3, 2, 3, 0, 1, 4, 4, 7, 1, 5, 1, 6, 5, 1, 6, 6, 6, 6, 0, 5, 5, 0, 1, 6, 0, 6, 1, 7, 2, 6, 1, 4, 1, 2, 4, 5, 4, 3, 1, 6, 6, 5, 3, 6, 1, 6, 1, 1, 7, 7, 5, 5, 2, 0, 0, 7, 4, 6, 4, 2, 2, 6, 4, 0, 5, 7, 5, 7, 0, 4, 2, 4, 2, 3, 2, 1, 3, 1, 7, 5, 5, 7, 5, 1, 5, 2, 1, 0, 4, 1, 5, 4, 5, 2, 7, 1, 4, 4, 7, 6, 1, 1, 5, 7, 7, 5, 6, 2, 5, 3, 0, 3, 6, 0, 1, 0, 3]\n",
            "1260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Generator"
      ],
      "metadata": {
        "id": "ss87vZorlHBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wise_Data_generator(Train_df,target,batch_size,steps):\n",
        "    idx=1\n",
        "    while True:\n",
        "        yield load_data(Train_df,target,idx-1,batch_size)## Yields data\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "\n",
        "\n",
        "\n",
        "def load_data(Train_df,target,idx,batch_size):\n",
        "    skiprows=idx*batch_size\n",
        "    nrows=batch_size\n",
        "    x = Train_df[skiprows:skiprows+nrows]\n",
        "    x = tf.keras.utils.normalize(x,axis=1)\n",
        "    #x = tf.expand_dims(x, 3)   # commented in Letnet-5\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    #x = tf.image.resize(x, [227,227]) # FOR ALEXNET\n",
        "    y = target[skiprows:skiprows+nrows]\n",
        "\n",
        "    return (np.array(x), np.array(y))"
      ],
      "metadata": {
        "id": "5DyeWaAolI_-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ],
      "metadata": {
        "id": "_i1N0ZVK_Czn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def printIndex(thelist):\n",
        "    thelist=thelist.tolist()\n",
        "    theset = frozenset(thelist)\n",
        "    theset = sorted(theset, reverse=True)\n",
        "    val= thelist.index(theset[0])\n",
        "    #print(val)\n",
        "    return val\n",
        "\n",
        "\n",
        "def getLabel_Probabilities(TestSample,model):\n",
        "  Total_Extracted_Val = []\n",
        "  TestSample=np.array(TestSample)\n",
        "  TestSample = tf.keras.utils.normalize(TestSample,axis=1)\n",
        "  layerIndex = 7  # FOR Lenet-5\n",
        "  func = K.function([model.get_layer(index=0).input], model.get_layer(index=layerIndex).output)\n",
        "  layerOutput = func([TestSample])  # input_data is a numpy array\n",
        "  len_output = layerOutput.shape[0]\n",
        "  for i in range(len_output):\n",
        "    k = printIndex(layerOutput[i])\n",
        "    Total_Extracted_Val.append(k)\n",
        "  return Total_Extracted_Val"
      ],
      "metadata": {
        "id": "1SoLEB7V_Efe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Out-of-band Patched data"
      ],
      "metadata": {
        "id": "FTKYdzj0-ZS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mal_data_synthesis(shape, data,repeatedSamples=1):\n",
        "    print(\"Original Data Shape\",shape)\n",
        "    lenOfData  = len(data)\n",
        "    print('total Patch will be loaded: ',lenOfData,\"\\nEach pattern will have \",repeatedSamples,\" replicas\")\n",
        "    input_shape = shape\n",
        "    mal_x = []\n",
        "    mal_y = []\n",
        "    Patch_test_x = []\n",
        "    Patch_test_y = []\n",
        "\n",
        "    # Two Pixel Points\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    k = 0\n",
        "\n",
        "    for b in range(2,60,1):  #Twelve Pixel Points to 20 pixel points\n",
        "        i=0\n",
        "        while(j<lenOfData and i<784):\n",
        "              target = data[j]\n",
        "              x = np.zeros(input_shape[1:]).reshape(1, 784)\n",
        "              x[0, i] = j / 255 + 1.0    # i goes up to 784\n",
        "\n",
        "              points = b\n",
        "              z1 = 2\n",
        "              for z in range(1,points,1):\n",
        "                x[0,(i+z)%784] = (z1+(k%2)) + 1.0\n",
        "                z1+=2\n",
        "\n",
        "              for d in range(repeatedSamples):\n",
        "                mal_x.append(copy.deepcopy(x))\n",
        "                mal_y.append(target)\n",
        "              Patch_test_x.append(x)\n",
        "              Patch_test_y.append(target)\n",
        "              i+=1\n",
        "              j+=1\n",
        "              k+=1\n",
        "\n",
        "\n",
        "    if(j<lenOfData):\n",
        "      print(\"FINISHED USING ALL Eleven PIXELS:  Adopt more patches, currently used \",len(Patch_test_x))\n",
        "\n",
        "\n",
        "    mal_x = np.asarray(mal_x, dtype=np.float32)\n",
        "    Patch_test_x = np.asarray(Patch_test_x, dtype=np.float32)\n",
        "    mal_y = np.asarray(mal_y, dtype=np.int32)\n",
        "    shape = [-1] + list(input_shape[1:])\n",
        "    mal_x = mal_x.reshape(shape)\n",
        "    Patch_test_x = Patch_test_x.reshape(shape)\n",
        "    Patch_test_y = np.asarray(Patch_test_y, dtype=np.int32)\n",
        "    #print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape)\n",
        "    print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape,\"\\nPatch_test_x shape: \",Patch_test_x.shape)\n",
        "\n",
        "    return mal_x, mal_y,Patch_test_x,Patch_test_y"
      ],
      "metadata": {
        "id": "-btMTaja-S-7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mal_x, mal_y,Patch_test_x,Patch_test_y = mal_data_synthesis(train_DATA_X.shape,Data,repeatedSamples = 1)\n",
        "print('Malicious Data :',mal_x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnc2jQUV-x8B",
        "outputId": "3293aa0a-ebbb-4424-ca5b-4c2f716c11dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape (12000, 28, 28)\n",
            "total Patch will be loaded:  1260 \n",
            "Each pattern will have  1  replicas\n",
            "mal_x shape:  (1260, 28, 28)  mal_y shape:  (1260,) \n",
            "Patch_test_x shape:  (1260, 28, 28)\n",
            "Malicious Data : (1260, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = tf.keras.models.load_model('NonCovert_1260.h5')\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dA84WSY-4So",
        "outputId": "447f80aa-0fd7-4552-bd74-e3f8c025bf41"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 2s 2ms/step - loss: 0.1414 - accuracy: 0.9818\n",
            "loss and accuracy of Raw MNIST Data is 0.14142750203609467 0.9818000197410583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLvaEQ6p-8O6",
        "outputId": "2d315642-bc34-4936-c68f-05dcec730e59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260 1260\n",
            "Total incorrect before decoding:  0\n",
            "Extraction Top-1 Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Re-Training"
      ],
      "metadata": {
        "id": "0_NW2rtq-ccV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retraining(train_X, train_y, model, batch_size, epochs, validation_split):\n",
        "  '''\n",
        "  Args:\n",
        "      batch_size: size of a single batch\n",
        "      epochs: number of epochs\n",
        "      validation_split: percentange of training set will be used for validation set\n",
        "  Returns:\n",
        "      model_for_retraining: retrained model\n",
        "  '''\n",
        "  tf.keras.models.save_model(model, 'original')\n",
        "  model_for_retraining = model\n",
        "  steps_per_epoch=np.ceil(len(train_X)/batch_size)\n",
        "  my_training_batch_generator = batch_wise_Data_generator(train_X,train_y, batch_size,steps_per_epoch)\n",
        "  model_for_retraining.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "  model_for_retraining.fit(my_training_batch_generator,epochs=epochs,steps_per_epoch=steps_per_epoch,verbose=1)\n",
        "  #model_for_retraining.fit(train_X, train_y,\n",
        "                  #batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
        "  return tf.keras.models.load_model('original'), model_for_retraining"
      ],
      "metadata": {
        "id": "0GiNP8aElDtH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Pruning"
      ],
      "metadata": {
        "id": "urdGTx7jemgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the saved model\n",
        "\n",
        "model = tf.keras.models.load_model('NonCovert_1260.h5')\n",
        "\n",
        "# Load your validation dataset (for this example, we'll use CIFAR-10 as a clean dataset)\n",
        "x_train, y_train, x_test, y_test = train_DATA_X,train_DATA_Y, TEST_BASELINE_X,TEST_BASELINE_Y\n",
        "\n",
        "x_test = tf.keras.utils.normalize(x_test,axis=1)\n",
        "\n",
        "x_train = tf.keras.utils.normalize(x_train,axis=1)\n",
        "\n",
        "# Define pruning ratio (e.g., prune 10% of neurons)\n",
        "prune_ratio = 0.40  # Prune 10% of neurons in Dense layers"
      ],
      "metadata": {
        "id": "-ZvEUiO5DQOa"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_average_neuron_activations(model, data):\n",
        "    \"\"\"\n",
        "    Calculate the average activations for each neuron in Dense layers.\n",
        "    Args:\n",
        "        model: The DNN model.\n",
        "        data: The input data (e.g., validation dataset) to pass through the model.\n",
        "    Returns:\n",
        "        avg_activations: A list of average activations for each Dense layer's neurons.\n",
        "    \"\"\"\n",
        "    avg_activations_dense = []\n",
        "\n",
        "    # Extract only the outputs of Dense layers\n",
        "    dense_layers = [layer.output for layer in model.layers if isinstance(layer, tf.keras.layers.Dense)]\n",
        "\n",
        "    # Create an intermediate model to get the activations from Dense layers\n",
        "    intermediate_model = tf.keras.Model(inputs=model.input, outputs=dense_layers)\n",
        "\n",
        "    # Pass the data through the intermediate model to get activations\n",
        "    activations = intermediate_model.predict(data)\n",
        "\n",
        "    # Loop through each Dense layer's activations\n",
        "    for layer_activations in activations:\n",
        "        # layer_activations is of shape (batch_size, neurons), so we take the mean over the batch axis (axis=0)\n",
        "        avg_activation = np.mean(layer_activations, axis=0)  # Mean absolute activation for each neuron\n",
        "        avg_activations_dense.append(avg_activation)\n",
        "\n",
        "\n",
        "\n",
        "    # FOr Conv2D layer for Lenet5\n",
        "\n",
        "    avg_activations_conv2D = []\n",
        "\n",
        "    # Extract only the outputs of Conv2D layers\n",
        "    conv2d_layers = [layer.output for layer in model.layers if isinstance(layer, tf.keras.layers.Conv2D)]\n",
        "\n",
        "    # Create an intermediate model to get the activations from Conv2D layers\n",
        "    intermediate_model = tf.keras.Model(inputs=model.input, outputs=conv2d_layers)\n",
        "\n",
        "    # Pass the data through the intermediate model to get activations\n",
        "    activations = intermediate_model.predict(data)\n",
        "\n",
        "    # Loop through each Conv2D layer's activations\n",
        "    for layer_activations in activations:\n",
        "        # layer_activations is of shape (batch_size, height, width, num_filters)\n",
        "        # Calculate the mean over batch_size, height, and width for each filter\n",
        "        avg_activation = np.mean(layer_activations, axis=(0, 1, 2))  # Mean activation per filter\n",
        "        avg_activations_conv2D.append(avg_activation)\n",
        "\n",
        "    return avg_activations_dense,avg_activations_conv2D\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def prune_neurons_in_dense_layers(model, avg_neuron_activations_dense,avg_neuron_activations_conv2D,prune_ratio):\n",
        "    \"\"\"\n",
        "    Prune neurons in Dense layers based on their average activations.\n",
        "    Args:\n",
        "        model: the DNN model to be pruned.\n",
        "        avg_neuron_activations: a list of average activations for each Dense layer's neurons.\n",
        "        prune_ratio: fraction of neurons to prune (e.g., 0.1 for pruning 10% of neurons).\n",
        "    Returns:\n",
        "        pruned_model: the pruned model.\n",
        "    \"\"\"\n",
        "    pruned_model = tf.keras.models.clone_model(model)\n",
        "    val1=0\n",
        "    val2=0\n",
        "\n",
        "    # Loop through each Dense layer to prune neurons\n",
        "    for layer_idx, layer in enumerate(pruned_model.layers):\n",
        "\n",
        "        if isinstance(layer, tf.keras.layers.Dense):\n",
        "            # Get the number of neurons in the layer\n",
        "            num_neurons = len(avg_neuron_activations_dense[val1])\n",
        "            print(f\"Pruning layer {layer.name} with {num_neurons} neurons.\")\n",
        "\n",
        "            # Get the sorted indices of neurons based on average activations (low to high)\n",
        "            neuron_indices = np.argsort(avg_neuron_activations_dense[val1])\n",
        "\n",
        "            # Calculate how many neurons to prune\n",
        "            num_neurons_to_prune = int(prune_ratio * num_neurons)\n",
        "            if num_neurons_to_prune == 0:\n",
        "                print(f\"Skipping pruning for layer {layer.name} as prune_ratio results in 0 neurons to prune.\")\n",
        "                continue\n",
        "\n",
        "            neurons_to_prune = neuron_indices[:num_neurons_to_prune]\n",
        "\n",
        "            # Get current weights and biases\n",
        "            weights, biases = layer.get_weights()\n",
        "            #print(\"weights\",weights.shape)\n",
        "\n",
        "            # Prune Dense layer neurons (set the weights and biases of neurons_to_prune to zero)\n",
        "            weights[:, neurons_to_prune] = 0\n",
        "            biases[neurons_to_prune] = 0\n",
        "\n",
        "            # Set the pruned weights and biases\n",
        "            layer.set_weights([weights, biases])\n",
        "            print(f\"Pruned {num_neurons_to_prune} neurons in Dense layer {layer.name}\")\n",
        "            val1+=1\n",
        "\n",
        "        elif isinstance(layer, tf.keras.layers.Conv2D):\n",
        "            # Get the number of filters in the layer\n",
        "            num_filters = len(avg_neuron_activations_conv2D[val2])\n",
        "            print(f\"Pruning layer {layer.name} with {num_filters} filters.\")\n",
        "\n",
        "            # Get the sorted indices of filters based on average activations (low to high)\n",
        "            filter_indices = np.argsort(avg_neuron_activations_conv2D[val2])\n",
        "\n",
        "            # Calculate how many filters to prune\n",
        "            num_filters_to_prune = int(prune_ratio * num_filters)\n",
        "            if num_filters_to_prune == 0:\n",
        "                print(f\"Skipping pruning for layer {layer.name} as prune_ratio results in 0 filters to prune.\")\n",
        "                continue\n",
        "\n",
        "            filters_to_prune = filter_indices[:num_filters_to_prune]\n",
        "\n",
        "            # Get current weights and biases\n",
        "            weights, biases = layer.get_weights()\n",
        "            # weights shape: (height, width, input_channels, output_channels)\n",
        "\n",
        "            # Prune Conv2D filters (set the filters corresponding to filters_to_prune to zero)\n",
        "            weights[:, :, :, filters_to_prune] = 0  # Zero out the weights for pruned filters\n",
        "            biases[filters_to_prune] = 0  # Zero out the biases for pruned filters\n",
        "\n",
        "            # Set the pruned weights and biases\n",
        "            layer.set_weights([weights, biases])\n",
        "            print(f\"Pruned {num_filters_to_prune} filters in Conv2D layer {layer.name}\")\n",
        "\n",
        "            val2 += 1\n",
        "\n",
        "    return pruned_model\n",
        "\n",
        "\n",
        "# Calculate average activations (assuming you already have the function for this)\n",
        "avg_neuron_activations_dense,avg_neuron_activations_conv2D = get_average_neuron_activations(model, x_train)\n",
        "\n",
        "\n",
        "# Prune only Dense layers\n",
        "pruned_model = prune_neurons_in_dense_layers(model, avg_neuron_activations_dense,avg_neuron_activations_conv2D, prune_ratio)\n",
        "\n",
        "# Save the pruned model\n",
        "\n",
        "print(\"Pruned Dense model saved successfully.\")\n",
        "\n",
        "pruned_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "\n",
        "# Measure the accuracy after pruning\n",
        "current_accuracy = pruned_model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "\n",
        "print(f\"Pruning stopped. Final accuracy: {current_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHCs8PvAfbbz",
        "outputId": "9c20753c-d379-4d1e-f131-a3eeda11c0d1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 1s 2ms/step\n",
            "375/375 [==============================] - 1s 2ms/step\n",
            "Pruning layer conv2d_9 with 6 filters.\n",
            "Pruned 2 filters in Conv2D layer conv2d_9\n",
            "Pruning layer conv2d_10 with 16 filters.\n",
            "Pruned 6 filters in Conv2D layer conv2d_10\n",
            "Pruning layer conv2d_11 with 120 filters.\n",
            "Pruned 48 filters in Conv2D layer conv2d_11\n",
            "Pruning layer dense_6 with 84 neurons.\n",
            "Pruned 33 neurons in Dense layer dense_6\n",
            "Pruning layer dense_7 with 10 neurons.\n",
            "Pruned 4 neurons in Dense layer dense_7\n",
            "Pruned Dense model saved successfully.\n",
            "Pruning stopped. Final accuracy: 0.1543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, model_FinePruning = retraining(train_DATA_X,train_DATA_Y,pruned_model,batch_size=64, epochs=10, validation_split=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH4AO0AYlQpr",
        "outputId": "2fb8f535-fd42-46c8-b4fd-461485860872"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "188/188 [==============================] - 2s 4ms/step - loss: 0.8341 - accuracy: 0.7445\n",
            "Epoch 2/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.3253 - accuracy: 0.9056\n",
            "Epoch 3/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.2272 - accuracy: 0.9327\n",
            "Epoch 4/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1726 - accuracy: 0.9462\n",
            "Epoch 5/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.9583\n",
            "Epoch 6/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9673\n",
            "Epoch 7/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0916 - accuracy: 0.9738\n",
            "Epoch 8/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0748 - accuracy: 0.9793\n",
            "Epoch 9/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0614 - accuracy: 0.9846\n",
            "Epoch 10/10\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0505 - accuracy: 0.9891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "\n",
        "T_X = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model_FinePruning.evaluate(T_X,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pv-8-Evh3Gk",
        "outputId": "5cae73bc-b24d-469e-bcb7-b6c64a52b602"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1342 - accuracy: 0.9601\n",
            "loss and accuracy of Raw MNIST Data is 0.13420310616493225 0.960099995136261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Extracted_Val = getLabel_Probabilities(copy.deepcopy(Patch_test_x),model_FinePruning) # query the model\n",
        "\n",
        "\n",
        "sender_val=copy.deepcopy(Data)\n",
        "\n",
        "print(len(Total_Extracted_Val), len(sender_val))\n",
        "\n",
        "incorrect=0\n",
        "for i in range(len(sender_val)):\n",
        "  if(sender_val[i]!=Total_Extracted_Val[i]):\n",
        "    incorrect+=1\n",
        "\n",
        "print('Total incorrect before decoding: ',incorrect)\n",
        "TOP1_accuracy= (len(sender_val)-incorrect)/len(sender_val)\n",
        "TOP1_ErrorCount = incorrect\n",
        "print('Extraction Top-1 Accuracy: ',TOP1_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFrCFRKlh7Z_",
        "outputId": "21435614-ec5a-4300-c5a3-b021047f7d2b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260 1260\n",
            "Total incorrect before decoding:  1116\n",
            "Extraction Top-1 Accuracy:  0.11428571428571428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAMPp8kbu_zh"
      },
      "execution_count": 70,
      "outputs": []
    }
  ]
}